{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4bd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083ed44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nitrogen atom energy:  5.10 eV\n",
      "Nitrogen molecule energy:  0.44 eV\n",
      "Atomization energy:  9.76 eV\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "from ase.calculators.emt import EMT\n",
    "\n",
    "atom = Atoms('N')\n",
    "atom.calc = EMT()\n",
    "e_atom = atom.get_potential_energy()\n",
    "\n",
    "d = 1.1\n",
    "molecule = Atoms('2N', [(0., 0., 0.), (0., 0., d)])\n",
    "molecule.calc = EMT()\n",
    "e_molecule = molecule.get_potential_energy()\n",
    "\n",
    "e_atomization = e_molecule - 2 * e_atom\n",
    "\n",
    "print('Nitrogen atom energy: %5.2f eV' % e_atom)\n",
    "print('Nitrogen molecule energy: %5.2f eV' % e_molecule)\n",
    "print('Atomization energy: %5.2f eV' % -e_atomization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9b1ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\nequip\\utils\\_global_options.py:59: UserWarning: Setting the GLOBAL value for jit fusion strategy to `[('DYNAMIC', 3)]` which is different than the previous value of `[('STATIC', 2), ('DYNAMIC', 10)]`\n",
      "  f\"Setting the GLOBAL value for jit fusion strategy to `{new_strat}` which is different than the previous value of `{old_strat}`\"\n"
     ]
    }
   ],
   "source": [
    "import ase\n",
    "import nequip\n",
    "from nequip.ase import NequIPCalculator\n",
    "from ase import Atoms,  units\n",
    "from ase.io import read,write\n",
    "from ase.visualize import view\n",
    "from ase.io.trajectory import Trajectory\n",
    "import numpy as np\n",
    "from ase.geometry import get_layers\n",
    "from ase.build import surface, bulk\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution, Stationary, ZeroRotation\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "from ase.cluster.cubic import FaceCenteredCubic\n",
    "import numpy as np\n",
    "import torch\n",
    "from ase.io import read, write\n",
    "from ase.md import VelocityVerlet, Langevin\n",
    "from ase import units\n",
    "import random\n",
    "\n",
    "substrate_to_use=\"rutile\"\n",
    "nanoparticlesize=\"small\"\n",
    "temp = 300 #424\n",
    "oxy=5\n",
    "\n",
    "#set calculator\n",
    "calculator = NequIPCalculator.from_deployed_model(\n",
    "    model_path=\"8potbestnn.pth\",\n",
    "    species_to_type_name = {\n",
    "        \"Au\": \"Au\",\n",
    "        \"Ti\": \"Ti\",\n",
    "        \"O\": \"O\"\n",
    "    },\n",
    "    # energy_units_to_eV=0.043,\n",
    "    device='cpu')\n",
    "\n",
    "\n",
    "##struc\n",
    "if substrate_to_use==\"rutile\":\n",
    "    ######## rutile 101\n",
    "    atoms = read(\"TiO2rutile.cif\")\n",
    "    s=surface(atoms,(1,0,1),3,vacuum=10)\n",
    "    s.translate([0,0,-10])\n",
    "    news = s.repeat((7,7,1))\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"Ti\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"Ti\"].positions[::,2]).round(2)]\n",
    "    listofatoms = (news.positions[::,2]).round(2)==max(news[news.symbols==\"O\"].positions[::,2]).round(2)\n",
    "    listofatomsb = (news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)\n",
    "    for idx,i in enumerate(listofatoms):\n",
    "        if i==True:\n",
    "            listofatoms[idx]=random.choices([True, False],weights=(100-oxy,oxy))[0]\n",
    "\n",
    "    for idx,i in enumerate(listofatomsb):\n",
    "        if i==True:\n",
    "            listofatomsb[idx]=random.choices([True, False],weights=(100-oxy,oxy))[0]\n",
    "\n",
    "    del news[listofatomsb+listofatoms]\n",
    "\n",
    "    substrate = news\n",
    "    substraterutile = substrate \n",
    "    zsurface  = max(news.positions[::,2])\n",
    "    zcell = substrate.cell[2,2]\n",
    "    ########\n",
    "    substrate =substraterutile\n",
    "\n",
    "if substrate_to_use==\"anatase\":\n",
    "    ##atanatase 101 surfaces\n",
    "    atoms = read(\"TiO2_mp-390_conventional_standard.cif\")\n",
    "    s=surface(atoms,(1,0,1),8,vacuum=10)\n",
    "    s.translate([0,0,-10])\n",
    "\n",
    "    news = s.repeat((8,16,1))\n",
    "\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"Ti\"].positions[::,2]).round(2)]\n",
    "    del news[(news.positions[::,2]).round(2)==min(news[news.symbols==\"Ti\"].positions[::,2]).round(2)]\n",
    "\n",
    "    listofatoms = (news.positions[::,2]).round(2)==max(news[news.symbols==\"O\"].positions[::,2]).round(2)\n",
    "    listofatomsb = (news.positions[::,2]).round(2)==min(news[news.symbols==\"O\"].positions[::,2]).round(2)\n",
    "\n",
    "    for idx,i in enumerate(listofatoms):\n",
    "        if i==True:\n",
    "            listofatoms[idx]=random.choices([True, False],weights=(100-oxy,oxy))[0]\n",
    "\n",
    "    for idx,i in enumerate(listofatomsb):\n",
    "        if i==True:\n",
    "            listofatomsb[idx]=random.choices([True, False],weights=(100-oxy,oxy))[0]\n",
    "\n",
    "    del news[listofatomsb+listofatoms]\n",
    "\n",
    "    substrate = news\n",
    "    substrateanatase = substrate \n",
    "    zsurface  = max(news.positions[::,2])\n",
    "    zcell = substrate.cell[2,2]\n",
    "    ##########################################################\n",
    "\n",
    "    substrate=substrateanatase\n",
    "    \n",
    "\n",
    "#nanoparticle\n",
    "\n",
    "surfaces = [(1, 1, 1),(-1,-1,-1),(1,0,0)] # These are the surfaces I will include\n",
    "if nanoparticlesize==\"small\":\n",
    "    layers=[4,2,5]\n",
    "if nanoparticlesize==\"medium\":\n",
    "    layers=[5,3,6]\n",
    "if nanoparticlesize==\"big\":\n",
    "    layers=[6,5,7]\n",
    "if nanoparticlesize==\"superbig\":\n",
    "    layers=[7,4,8]\n",
    "    \n",
    "    \n",
    "np = FaceCenteredCubic('Au', surfaces, layers)\n",
    "np.rotate((1,1,1),(0,0,1))\n",
    "np.cell = substrate.cell\n",
    "np.center()\n",
    "np.translate([0,0,1])\n",
    "np.rotate(v=(0,0,1),a=75,center='COU')\n",
    "\n",
    "\n",
    "znp  = min(np.positions[::,2])\n",
    "zsize = max(np.positions[::,2])-min(np.positions[::,2])\n",
    "np.translate([0,0,-znp+zsurface+2.0])\n",
    "znp  = min(np.positions[::,2])\n",
    "np.translate([0,0,-znp+zsurface+2.1])\n",
    "# np.rotate(v=(0,0,1),a=30,center='COU')\n",
    "\n",
    "atoms = substrate + np\n",
    "atoms.center(vacuum=5,axis=2)\n",
    "\n",
    "\n",
    "t1 = Trajectory('atoms_to_run.traj', 'a')\n",
    "t1.write(atoms)\n",
    "atoms = read(\"atoms_to_run.traj\")\n",
    "\n",
    "atoms.calc=calculator\n",
    "# del np \n",
    "# import numpy as np\n",
    "# temp = np.zeros((len(atoms),3))\n",
    "# for i in atoms:\n",
    "#     index=i.index\n",
    "#     if i.symbol==\"Au\":\n",
    "#         temp[index]=[600,600,600]\n",
    "#     else: temp[index]=[300,300,300]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# atoms.calc = calculator\n",
    "# atoms.pbc = [True,True,False]\n",
    "\n",
    "# trajectory = Trajectory(\"substrate{}_nanoparticle{}oxygen{}.traj\".format(substrate_to_use,nanoparticlesize,oxy), \"w\", atoms)\n",
    "\n",
    "# #MaxwellBoltzmannDistribution(atoms, temperature_K=1*temp) #give momenta to atoms according to 300K\n",
    "# #MaxwellBoltzmannDistribution(atoms, temperature_K=0.5*300) #give momenta to atoms according to 300K\n",
    "# dyn = Langevin(atoms, timestep=1*units.fs, temperature=temp*units.kB,friction=0.005)\n",
    "# dyn.attach(trajectory, interval=50)\n",
    "# dyn.run(1550)\n",
    "# del atoms[(atoms.positions[::,2]-atoms.cell[2,2]/2)**2>(atoms.cell[2,2]/2)**2]\n",
    "# del np\n",
    "# import numpy as np\n",
    "# temp = np.zeros((len(atoms),3))\n",
    "# for i in atoms:\n",
    "#     index=i.index\n",
    "#     if i.symbol==\"Au\":\n",
    "#         temp[index]=[600,600,600]\n",
    "#     else: temp[index]=[300,300,300]\n",
    "\n",
    "        \n",
    "# for i in range(30):\n",
    "#     dyn = Langevin(atoms, timestep=1*units.fs, temperature=temp*units.kB,friction=0.005)\n",
    "#     dyn.attach(trajectory, interval=100)\n",
    "#     dyn.run(500)\n",
    "#     del atoms[(atoms.positions[::,2]-atoms.cell[2,2]/2)**2>(atoms.cell[2,2]/2)**2]\n",
    "#     del np\n",
    "#     import numpy as np\n",
    "#     temp = np.zeros((len(atoms),3))\n",
    "#     for i in atoms:\n",
    "#         index=i.index\n",
    "#         if i.symbol==\"Au\":\n",
    "#             temp[index]=[600,600,600]\n",
    "#         else: temp[index]=[300,300,300]\n",
    "    \n",
    "# for i in range(300):\n",
    "#     dyn = VelocityVerlet(atoms, 2*units.fs)\n",
    "#     dyn.attach(trajectory, interval=50) \n",
    "#     dyn.run(500)\n",
    "#     del atoms[(atoms.positions[::,2]-atoms.cell[2,2]/2)**2>(atoms.cell[2,2]/2)**2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df3c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x287e48a9f98>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722880d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8696178  -1.7590066   1.7088649 ]\n",
      " [-1.4395887  -0.79026914  1.732662  ]\n",
      " [ 0.17353123  0.3963975  -0.0403138 ]\n",
      " ...\n",
      " [ 0.01232634 -0.21453097 -0.27160248]\n",
      " [-0.04746277 -0.08220717  0.6208082 ]\n",
      " [-0.00774002  0.01339289  0.02173525]]\n"
     ]
    }
   ],
   "source": [
    "print(atoms.get_forces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9578411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x287cf579d68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf1c39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuau_\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.30500674 12.01676696 57.02059978 57.02059978]\n",
      "Total number of atoms in the trajectory: 64\n",
      "this is the gnn FlexibleGNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (out_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1/1100, Loss: 837.2981719970703, Learning rate: 0.01\n",
      "Epoch 2/1100, Loss: 824.7055568695068, Learning rate: 0.01\n",
      "Epoch 3/1100, Loss: 913.5705585479736, Learning rate: 0.01\n",
      "Epoch 4/1100, Loss: 769.2488136291504, Learning rate: 0.01\n",
      "Epoch 5/1100, Loss: 720.9996528625488, Learning rate: 0.01\n",
      "Epoch 6/1100, Loss: 731.9228954315186, Learning rate: 0.01\n",
      "Epoch 7/1100, Loss: 743.6644172668457, Learning rate: 0.01\n",
      "Epoch 8/1100, Loss: 742.5925064086914, Learning rate: 0.01\n",
      "Epoch 9/1100, Loss: 736.6702709197998, Learning rate: 0.01\n",
      "Epoch 10/1100, Loss: 730.6513366699219, Learning rate: 0.01\n",
      "Epoch 11/1100, Loss: 727.5185375213623, Learning rate: 0.01\n",
      "Epoch 12/1100, Loss: 724.0990962982178, Learning rate: 0.01\n",
      "Epoch 13/1100, Loss: 721.0666255950928, Learning rate: 0.01\n",
      "Epoch 14/1100, Loss: 717.6472320556641, Learning rate: 0.01\n",
      "Epoch 15/1100, Loss: 715.2433528900146, Learning rate: 0.01\n",
      "Epoch 16/1100, Loss: 712.2133407592773, Learning rate: 0.01\n",
      "Epoch 17/1100, Loss: 710.8487854003906, Learning rate: 0.01\n",
      "Epoch 18/1100, Loss: 707.9136734008789, Learning rate: 0.01\n",
      "Epoch 19/1100, Loss: 706.8122634887695, Learning rate: 0.01\n",
      "Epoch 20/1100, Loss: 703.6910305023193, Learning rate: 0.01\n",
      "Epoch 21/1100, Loss: 704.0442867279053, Learning rate: 0.01\n",
      "Epoch 22/1100, Loss: 700.6462211608887, Learning rate: 0.01\n",
      "Epoch 23/1100, Loss: 700.949052810669, Learning rate: 0.01\n",
      "Epoch 24/1100, Loss: 696.7676429748535, Learning rate: 0.01\n",
      "Epoch 25/1100, Loss: 698.2103881835938, Learning rate: 0.01\n",
      "Epoch 26/1100, Loss: 694.3056545257568, Learning rate: 0.01\n",
      "Epoch 27/1100, Loss: 695.4524879455566, Learning rate: 0.01\n",
      "Epoch 28/1100, Loss: 691.0577239990234, Learning rate: 0.01\n",
      "Epoch 29/1100, Loss: 691.9704399108887, Learning rate: 0.01\n",
      "Epoch 30/1100, Loss: 688.6739559173584, Learning rate: 0.01\n",
      "Epoch 31/1100, Loss: 689.9209213256836, Learning rate: 0.01\n",
      "Epoch 32/1100, Loss: 685.7148342132568, Learning rate: 0.01\n",
      "Epoch 33/1100, Loss: 682.1029682159424, Learning rate: 0.01\n",
      "Epoch 34/1100, Loss: 681.1858825683594, Learning rate: 0.01\n",
      "Epoch 35/1100, Loss: 678.3830413818359, Learning rate: 0.01\n",
      "Epoch 36/1100, Loss: 684.2191638946533, Learning rate: 0.01\n",
      "Epoch 37/1100, Loss: 685.1411457061768, Learning rate: 0.01\n",
      "Epoch 38/1100, Loss: 677.3337249755859, Learning rate: 0.01\n",
      "Epoch 39/1100, Loss: 684.6258563995361, Learning rate: 0.01\n",
      "Epoch 40/1100, Loss: 680.5890674591064, Learning rate: 0.01\n",
      "Epoch 41/1100, Loss: 689.99582862854, Learning rate: 0.01\n",
      "Epoch 42/1100, Loss: 683.2837924957275, Learning rate: 0.01\n",
      "Epoch 43/1100, Loss: 682.3823318481445, Learning rate: 0.01\n",
      "Epoch 44/1100, Loss: 673.6034717559814, Learning rate: 0.01\n",
      "Epoch 45/1100, Loss: 653.379674911499, Learning rate: 0.01\n",
      "Epoch 46/1100, Loss: 686.2743530273438, Learning rate: 0.01\n",
      "Epoch 47/1100, Loss: 775.6471252441406, Learning rate: 0.01\n",
      "Epoch 48/1100, Loss: 743.7240905761719, Learning rate: 0.01\n",
      "Epoch 49/1100, Loss: 687.2324256896973, Learning rate: 0.01\n",
      "Epoch 50/1100, Loss: 675.6070652008057, Learning rate: 0.01\n",
      "Epoch 51/1100, Loss: 679.3460292816162, Learning rate: 0.01\n",
      "Epoch 52/1100, Loss: 680.6394748687744, Learning rate: 0.01\n",
      "Epoch 53/1100, Loss: 678.9164447784424, Learning rate: 0.01\n",
      "Epoch 54/1100, Loss: 676.8911228179932, Learning rate: 0.01\n",
      "Epoch 55/1100, Loss: 675.8928623199463, Learning rate: 0.01\n",
      "Epoch 56/1100, Loss: 674.7832698822021, Learning rate: 0.01\n",
      "Epoch 57/1100, Loss: 674.0563583374023, Learning rate: 0.01\n",
      "Epoch 58/1100, Loss: 672.0724964141846, Learning rate: 0.01\n",
      "Epoch 59/1100, Loss: 671.7505435943604, Learning rate: 0.01\n",
      "Epoch 60/1100, Loss: 671.2506408691406, Learning rate: 0.01\n",
      "Epoch 61/1100, Loss: 669.1785411834717, Learning rate: 0.01\n",
      "Epoch 62/1100, Loss: 665.3052520751953, Learning rate: 0.01\n",
      "Epoch 63/1100, Loss: 670.6431217193604, Learning rate: 0.01\n",
      "Epoch 64/1100, Loss: 672.9877166748047, Learning rate: 0.01\n",
      "Epoch 65/1100, Loss: 669.278392791748, Learning rate: 0.01\n",
      "Epoch 66/1100, Loss: 664.7355804443359, Learning rate: 0.01\n",
      "Epoch 67/1100, Loss: 666.7213439941406, Learning rate: 0.01\n",
      "Epoch 68/1100, Loss: 667.2502632141113, Learning rate: 0.01\n",
      "Epoch 69/1100, Loss: 664.0157089233398, Learning rate: 0.01\n",
      "Epoch 70/1100, Loss: 659.4324893951416, Learning rate: 0.01\n",
      "Epoch 71/1100, Loss: 661.4845123291016, Learning rate: 0.01\n",
      "Epoch 72/1100, Loss: 664.4230613708496, Learning rate: 0.01\n",
      "Epoch 73/1100, Loss: 667.1205558776855, Learning rate: 0.01\n",
      "Epoch 74/1100, Loss: 665.548152923584, Learning rate: 0.01\n",
      "Epoch 75/1100, Loss: 662.753303527832, Learning rate: 0.01\n",
      "Epoch 76/1100, Loss: 660.8090534210205, Learning rate: 0.01\n",
      "Epoch 77/1100, Loss: 658.3832836151123, Learning rate: 0.01\n",
      "Epoch 78/1100, Loss: 654.5576171875, Learning rate: 0.01\n",
      "Epoch 79/1100, Loss: 654.7398204803467, Learning rate: 0.01\n",
      "Epoch 80/1100, Loss: 655.8020133972168, Learning rate: 0.01\n",
      "Epoch 81/1100, Loss: 658.5642833709717, Learning rate: 0.01\n",
      "Epoch 82/1100, Loss: 655.333854675293, Learning rate: 0.01\n",
      "Epoch 83/1100, Loss: 651.7622222900391, Learning rate: 0.01\n",
      "Epoch 84/1100, Loss: 650.1418304443359, Learning rate: 0.01\n",
      "Epoch 85/1100, Loss: 647.85618019104, Learning rate: 0.01\n",
      "Epoch 86/1100, Loss: 647.3180484771729, Learning rate: 0.01\n",
      "Epoch 87/1100, Loss: 642.7331047058105, Learning rate: 0.01\n",
      "Epoch 88/1100, Loss: 646.8126602172852, Learning rate: 0.01\n",
      "Epoch 89/1100, Loss: 642.5905361175537, Learning rate: 0.01\n",
      "Epoch 90/1100, Loss: 650.1329460144043, Learning rate: 0.01\n",
      "Epoch 91/1100, Loss: 646.705415725708, Learning rate: 0.01\n",
      "Epoch 92/1100, Loss: 636.8746547698975, Learning rate: 0.01\n",
      "Epoch 93/1100, Loss: 642.8482971191406, Learning rate: 0.01\n",
      "Epoch 94/1100, Loss: 632.179033279419, Learning rate: 0.01\n",
      "Epoch 95/1100, Loss: 639.0725860595703, Learning rate: 0.01\n",
      "Epoch 96/1100, Loss: 636.9002075195312, Learning rate: 0.01\n",
      "Epoch 97/1100, Loss: 654.0271053314209, Learning rate: 0.01\n",
      "Epoch 98/1100, Loss: 645.9623165130615, Learning rate: 0.01\n",
      "Epoch 99/1100, Loss: 639.0704860687256, Learning rate: 0.01\n",
      "Epoch 100/1100, Loss: 643.227481842041, Learning rate: 0.01\n",
      "Epoch 101/1100, Loss: 637.6917190551758, Learning rate: 0.01\n",
      "Epoch 102/1100, Loss: 638.1703815460205, Learning rate: 0.01\n",
      "Epoch 103/1100, Loss: 625.8890571594238, Learning rate: 0.01\n",
      "Epoch 104/1100, Loss: 627.3711261749268, Learning rate: 0.01\n",
      "Epoch 105/1100, Loss: 618.5014305114746, Learning rate: 0.01\n",
      "Epoch 106/1100, Loss: 619.7098731994629, Learning rate: 0.01\n",
      "Epoch 107/1100, Loss: 603.7920608520508, Learning rate: 0.01\n",
      "Epoch 108/1100, Loss: 607.2019309997559, Learning rate: 0.01\n",
      "Epoch 109/1100, Loss: 590.4799995422363, Learning rate: 0.01\n",
      "Epoch 110/1100, Loss: 625.3987560272217, Learning rate: 0.01\n",
      "Epoch 111/1100, Loss: 620.5472793579102, Learning rate: 0.01\n",
      "Epoch 112/1100, Loss: 598.8276634216309, Learning rate: 0.01\n",
      "Epoch 113/1100, Loss: 593.598274230957, Learning rate: 0.01\n",
      "Epoch 114/1100, Loss: 575.5320949554443, Learning rate: 0.01\n",
      "Epoch 115/1100, Loss: 612.9637565612793, Learning rate: 0.004\n",
      "Epoch 116/1100, Loss: 586.5022029876709, Learning rate: 0.004\n",
      "Epoch 117/1100, Loss: 550.3141098022461, Learning rate: 0.004\n",
      "Epoch 118/1100, Loss: 570.8036289215088, Learning rate: 0.004\n",
      "Epoch 119/1100, Loss: 550.1874694824219, Learning rate: 0.004\n",
      "Epoch 120/1100, Loss: 558.0299205780029, Learning rate: 0.004\n",
      "Epoch 121/1100, Loss: 552.8245887756348, Learning rate: 0.004\n",
      "Epoch 122/1100, Loss: 548.7710361480713, Learning rate: 0.004\n",
      "Epoch 123/1100, Loss: 551.0139217376709, Learning rate: 0.004\n",
      "Epoch 124/1100, Loss: 546.179594039917, Learning rate: 0.004\n",
      "Epoch 125/1100, Loss: 548.5026760101318, Learning rate: 0.004\n",
      "Epoch 126/1100, Loss: 544.3444538116455, Learning rate: 0.004\n",
      "Epoch 127/1100, Loss: 543.0452518463135, Learning rate: 0.004\n",
      "Epoch 128/1100, Loss: 543.4849662780762, Learning rate: 0.004\n",
      "Epoch 129/1100, Loss: 539.083324432373, Learning rate: 0.004\n",
      "Epoch 130/1100, Loss: 539.8084239959717, Learning rate: 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/1100, Loss: 537.1973094940186, Learning rate: 0.004\n",
      "Epoch 132/1100, Loss: 536.6755790710449, Learning rate: 0.004\n",
      "Epoch 133/1100, Loss: 533.9571132659912, Learning rate: 0.004\n",
      "Epoch 134/1100, Loss: 533.3078155517578, Learning rate: 0.004\n",
      "Epoch 135/1100, Loss: 530.2440853118896, Learning rate: 0.004\n",
      "Epoch 136/1100, Loss: 528.1656513214111, Learning rate: 0.004\n",
      "Epoch 137/1100, Loss: 525.6216983795166, Learning rate: 0.004\n",
      "Epoch 138/1100, Loss: 524.0273303985596, Learning rate: 0.004\n",
      "Epoch 139/1100, Loss: 520.7013931274414, Learning rate: 0.004\n",
      "Epoch 140/1100, Loss: 519.3116397857666, Learning rate: 0.004\n",
      "Epoch 141/1100, Loss: 515.4420280456543, Learning rate: 0.004\n",
      "Epoch 142/1100, Loss: 515.3932666778564, Learning rate: 0.004\n",
      "Epoch 143/1100, Loss: 510.051794052124, Learning rate: 0.004\n",
      "Epoch 144/1100, Loss: 508.68957138061523, Learning rate: 0.004\n",
      "Epoch 145/1100, Loss: 502.02830696105957, Learning rate: 0.004\n",
      "Epoch 146/1100, Loss: 502.1048698425293, Learning rate: 0.004\n",
      "Epoch 147/1100, Loss: 495.3938522338867, Learning rate: 0.004\n",
      "Epoch 148/1100, Loss: 484.2240676879883, Learning rate: 0.004\n",
      "Epoch 149/1100, Loss: 507.54416275024414, Learning rate: 0.004\n",
      "Epoch 150/1100, Loss: 511.25510597229004, Learning rate: 0.004\n",
      "Epoch 151/1100, Loss: 502.6513042449951, Learning rate: 0.004\n",
      "Epoch 152/1100, Loss: 465.74650955200195, Learning rate: 0.004\n",
      "Epoch 153/1100, Loss: 469.72287368774414, Learning rate: 0.004\n",
      "Epoch 154/1100, Loss: 461.7304992675781, Learning rate: 0.004\n",
      "Epoch 155/1100, Loss: 521.253870010376, Learning rate: 0.004\n",
      "Epoch 156/1100, Loss: 542.9824085235596, Learning rate: 0.004\n",
      "Epoch 157/1100, Loss: 486.7562656402588, Learning rate: 0.004\n",
      "Epoch 158/1100, Loss: 442.5667133331299, Learning rate: 0.004\n",
      "Epoch 159/1100, Loss: 436.7754077911377, Learning rate: 0.004\n",
      "Epoch 160/1100, Loss: 452.716365814209, Learning rate: 0.004\n",
      "Epoch 161/1100, Loss: 411.7883834838867, Learning rate: 0.004\n",
      "Epoch 162/1100, Loss: 427.02918434143066, Learning rate: 0.004\n",
      "Epoch 163/1100, Loss: 424.69379329681396, Learning rate: 0.004\n",
      "Epoch 164/1100, Loss: 494.9475622177124, Learning rate: 0.004\n",
      "Epoch 165/1100, Loss: 508.9879455566406, Learning rate: 0.004\n",
      "Epoch 166/1100, Loss: 472.7317752838135, Learning rate: 0.004\n",
      "Epoch 167/1100, Loss: 398.16951179504395, Learning rate: 0.004\n",
      "Epoch 168/1100, Loss: 366.02240085601807, Learning rate: 0.004\n",
      "Epoch 169/1100, Loss: 368.9469881057739, Learning rate: 0.004\n",
      "Epoch 170/1100, Loss: 340.04719161987305, Learning rate: 0.004\n",
      "Epoch 171/1100, Loss: 340.7470693588257, Learning rate: 0.004\n",
      "Epoch 172/1100, Loss: 322.0087652206421, Learning rate: 0.004\n",
      "Epoch 173/1100, Loss: 332.7284116744995, Learning rate: 0.004\n",
      "Epoch 174/1100, Loss: 383.35333156585693, Learning rate: 0.004\n",
      "Epoch 175/1100, Loss: 486.9842119216919, Learning rate: 0.004\n",
      "Epoch 176/1100, Loss: 576.9340591430664, Learning rate: 0.004\n",
      "Epoch 177/1100, Loss: 491.37879943847656, Learning rate: 0.004\n",
      "Epoch 178/1100, Loss: 324.34525299072266, Learning rate: 0.004\n",
      "Epoch 179/1100, Loss: 327.2296676635742, Learning rate: 0.004\n",
      "Epoch 180/1100, Loss: 340.749737739563, Learning rate: 0.004\n",
      "Epoch 181/1100, Loss: 341.88824367523193, Learning rate: 0.004\n",
      "Epoch 182/1100, Loss: 309.7055416107178, Learning rate: 0.004\n",
      "Epoch 183/1100, Loss: 290.82125663757324, Learning rate: 0.004\n",
      "Epoch 184/1100, Loss: 272.3712844848633, Learning rate: 0.004\n",
      "Epoch 185/1100, Loss: 245.30348205566406, Learning rate: 0.004\n",
      "Epoch 186/1100, Loss: 215.15547370910645, Learning rate: 0.004\n",
      "Epoch 187/1100, Loss: 217.92908143997192, Learning rate: 0.004\n",
      "Epoch 188/1100, Loss: 179.1182804107666, Learning rate: 0.004\n",
      "Epoch 189/1100, Loss: 196.0556960105896, Learning rate: 0.004\n",
      "Epoch 190/1100, Loss: 147.3760266304016, Learning rate: 0.004\n",
      "Epoch 191/1100, Loss: 193.71358156204224, Learning rate: 0.004\n",
      "Epoch 192/1100, Loss: 109.78333377838135, Learning rate: 0.004\n",
      "Epoch 193/1100, Loss: 201.2383954524994, Learning rate: 0.004\n",
      "Epoch 194/1100, Loss: 106.43091797828674, Learning rate: 0.004\n",
      "Epoch 195/1100, Loss: 248.36364889144897, Learning rate: 0.004\n",
      "Epoch 196/1100, Loss: 112.61191058158875, Learning rate: 0.004\n",
      "Epoch 197/1100, Loss: 212.6120743751526, Learning rate: 0.004\n",
      "Epoch 198/1100, Loss: 146.4000141620636, Learning rate: 0.004\n",
      "Epoch 199/1100, Loss: 322.0153305530548, Learning rate: 0.004\n",
      "Epoch 200/1100, Loss: 255.07889461517334, Learning rate: 0.004\n",
      "Epoch 201/1100, Loss: 252.25128078460693, Learning rate: 0.004\n",
      "Epoch 202/1100, Loss: 334.98584508895874, Learning rate: 0.004\n",
      "Epoch 203/1100, Loss: 218.15199446678162, Learning rate: 0.004\n",
      "Epoch 204/1100, Loss: 461.7847943305969, Learning rate: 0.004\n",
      "Epoch 205/1100, Loss: 450.4334125518799, Learning rate: 0.004\n",
      "Epoch 206/1100, Loss: 733.8063597679138, Learning rate: 0.004\n",
      "Epoch 207/1100, Loss: 479.37179470062256, Learning rate: 0.004\n",
      "Epoch 208/1100, Loss: 338.91200017929077, Learning rate: 0.004\n",
      "Epoch 209/1100, Loss: 178.15323686599731, Learning rate: 0.004\n",
      "Epoch 210/1100, Loss: 172.42575693130493, Learning rate: 0.004\n",
      "Epoch 211/1100, Loss: 176.866201877594, Learning rate: 0.004\n",
      "Epoch 212/1100, Loss: 161.79692935943604, Learning rate: 0.004\n",
      "Epoch 213/1100, Loss: 143.5688910484314, Learning rate: 0.004\n",
      "Epoch 214/1100, Loss: 133.83732748031616, Learning rate: 0.004\n",
      "Epoch 215/1100, Loss: 143.44354581832886, Learning rate: 0.004\n",
      "Epoch 216/1100, Loss: 146.1747260093689, Learning rate: 0.004\n",
      "Epoch 217/1100, Loss: 140.70853304862976, Learning rate: 0.004\n",
      "Epoch 218/1100, Loss: 173.553368806839, Learning rate: 0.004\n",
      "Epoch 219/1100, Loss: 257.62958312034607, Learning rate: 0.004\n",
      "Epoch 220/1100, Loss: 313.30145049095154, Learning rate: 0.004\n",
      "Epoch 221/1100, Loss: 260.7241702079773, Learning rate: 0.004\n",
      "Epoch 222/1100, Loss: 109.22668766975403, Learning rate: 0.004\n",
      "Epoch 223/1100, Loss: 110.51936864852905, Learning rate: 0.004\n",
      "Epoch 224/1100, Loss: 127.6374044418335, Learning rate: 0.004\n",
      "Epoch 225/1100, Loss: 194.72336339950562, Learning rate: 0.004\n",
      "Epoch 226/1100, Loss: 117.0656623840332, Learning rate: 0.004\n",
      "Epoch 227/1100, Loss: 70.03416681289673, Learning rate: 0.004\n",
      "Epoch 228/1100, Loss: 79.58899474143982, Learning rate: 0.004\n",
      "Epoch 229/1100, Loss: 104.6857099533081, Learning rate: 0.004\n",
      "Epoch 230/1100, Loss: 142.5877194404602, Learning rate: 0.0016\n",
      "Epoch 231/1100, Loss: 64.75348722934723, Learning rate: 0.0016\n",
      "Epoch 232/1100, Loss: 60.42935395240784, Learning rate: 0.0016\n",
      "Epoch 233/1100, Loss: 52.95625710487366, Learning rate: 0.0016\n",
      "Epoch 234/1100, Loss: 52.47871935367584, Learning rate: 0.0016\n",
      "Epoch 235/1100, Loss: 49.330162525177, Learning rate: 0.0016\n",
      "Epoch 236/1100, Loss: 49.510693430900574, Learning rate: 0.0016\n",
      "Epoch 237/1100, Loss: 48.23497927188873, Learning rate: 0.0016\n",
      "Epoch 238/1100, Loss: 47.13002908229828, Learning rate: 0.0016\n",
      "Epoch 239/1100, Loss: 46.09254479408264, Learning rate: 0.0016\n",
      "Epoch 240/1100, Loss: 45.10390067100525, Learning rate: 0.0016\n",
      "Epoch 241/1100, Loss: 44.45924150943756, Learning rate: 0.0016\n",
      "Epoch 242/1100, Loss: 43.87341344356537, Learning rate: 0.0016\n",
      "Epoch 243/1100, Loss: 43.28917336463928, Learning rate: 0.0016\n",
      "Epoch 244/1100, Loss: 42.690821051597595, Learning rate: 0.0016\n",
      "Epoch 245/1100, Loss: 42.14570963382721, Learning rate: 0.0016\n",
      "Epoch 246/1100, Loss: 41.6211895942688, Learning rate: 0.0016\n",
      "Epoch 247/1100, Loss: 41.112550377845764, Learning rate: 0.0016\n",
      "Epoch 248/1100, Loss: 40.564293682575226, Learning rate: 0.0016\n",
      "Epoch 249/1100, Loss: 40.08931654691696, Learning rate: 0.0016\n",
      "Epoch 250/1100, Loss: 39.82853984832764, Learning rate: 0.0016\n",
      "Epoch 251/1100, Loss: 39.14967322349548, Learning rate: 0.0016\n",
      "Epoch 252/1100, Loss: 39.682727694511414, Learning rate: 0.0016\n",
      "Epoch 253/1100, Loss: 42.719219744205475, Learning rate: 0.0016\n",
      "Epoch 254/1100, Loss: 40.39165359735489, Learning rate: 0.0016\n",
      "Epoch 255/1100, Loss: 38.03572237491608, Learning rate: 0.0016\n",
      "Epoch 256/1100, Loss: 40.263845920562744, Learning rate: 0.0016\n",
      "Epoch 257/1100, Loss: 40.15308743715286, Learning rate: 0.0016\n",
      "Epoch 258/1100, Loss: 37.081106781959534, Learning rate: 0.0016\n",
      "Epoch 259/1100, Loss: 38.23424756526947, Learning rate: 0.0016\n",
      "Epoch 260/1100, Loss: 40.54202103614807, Learning rate: 0.0016\n",
      "Epoch 261/1100, Loss: 37.94690719246864, Learning rate: 0.0016\n",
      "Epoch 262/1100, Loss: 35.312329947948456, Learning rate: 0.0016\n",
      "Epoch 263/1100, Loss: 38.36323854327202, Learning rate: 0.0016\n",
      "Epoch 264/1100, Loss: 39.68071189522743, Learning rate: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/1100, Loss: 34.54797399044037, Learning rate: 0.0016\n",
      "Epoch 266/1100, Loss: 31.943633675575256, Learning rate: 0.0016\n",
      "Epoch 267/1100, Loss: 35.616115272045135, Learning rate: 0.0016\n",
      "Epoch 268/1100, Loss: 37.77658358216286, Learning rate: 0.0016\n",
      "Epoch 269/1100, Loss: 30.26192796230316, Learning rate: 0.0016\n",
      "Epoch 270/1100, Loss: 30.55789178609848, Learning rate: 0.0016\n",
      "Epoch 271/1100, Loss: 37.326028764247894, Learning rate: 0.0016\n",
      "Epoch 272/1100, Loss: 42.16181343793869, Learning rate: 0.0016\n",
      "Epoch 273/1100, Loss: 32.31054323911667, Learning rate: 0.0016\n",
      "Epoch 274/1100, Loss: 28.976816326379776, Learning rate: 0.0016\n",
      "Epoch 275/1100, Loss: 35.301265984773636, Learning rate: 0.0016\n",
      "Epoch 276/1100, Loss: 36.37650844454765, Learning rate: 0.0016\n",
      "Epoch 277/1100, Loss: 26.732350289821625, Learning rate: 0.0016\n",
      "Epoch 278/1100, Loss: 29.06078776717186, Learning rate: 0.0016\n",
      "Epoch 279/1100, Loss: 38.46218687295914, Learning rate: 0.0016\n",
      "Epoch 280/1100, Loss: 31.22746181488037, Learning rate: 0.0016\n",
      "Epoch 281/1100, Loss: 27.003864228725433, Learning rate: 0.0016\n",
      "Epoch 282/1100, Loss: 28.256843090057373, Learning rate: 0.0016\n",
      "Epoch 283/1100, Loss: 28.781083464622498, Learning rate: 0.0016\n",
      "Epoch 284/1100, Loss: 30.655107587575912, Learning rate: 0.0016\n",
      "Epoch 285/1100, Loss: 27.949290454387665, Learning rate: 0.0016\n",
      "Epoch 286/1100, Loss: 24.884223610162735, Learning rate: 0.0016\n",
      "Epoch 287/1100, Loss: 28.061147540807724, Learning rate: 0.0016\n",
      "Epoch 288/1100, Loss: 36.2118743956089, Learning rate: 0.0016\n",
      "Epoch 289/1100, Loss: 28.870026499032974, Learning rate: 0.0016\n",
      "Epoch 290/1100, Loss: 23.754437118768692, Learning rate: 0.0016\n",
      "Epoch 291/1100, Loss: 26.16729387640953, Learning rate: 0.0016\n",
      "Epoch 292/1100, Loss: 26.39259821176529, Learning rate: 0.0016\n",
      "Epoch 293/1100, Loss: 22.31756889820099, Learning rate: 0.0016\n",
      "Epoch 294/1100, Loss: 25.928920030593872, Learning rate: 0.0016\n",
      "Epoch 295/1100, Loss: 28.379410535097122, Learning rate: 0.0016\n",
      "Epoch 296/1100, Loss: 26.109646022319794, Learning rate: 0.0016\n",
      "Epoch 297/1100, Loss: 23.45166912674904, Learning rate: 0.0016\n",
      "Epoch 298/1100, Loss: 21.615012481808662, Learning rate: 0.0016\n",
      "Epoch 299/1100, Loss: 25.441482231020927, Learning rate: 0.0016\n",
      "Epoch 300/1100, Loss: 24.253742188215256, Learning rate: 0.0016\n",
      "Epoch 301/1100, Loss: 21.389835625886917, Learning rate: 0.0016\n",
      "Epoch 302/1100, Loss: 22.078495413064957, Learning rate: 0.0016\n",
      "Epoch 303/1100, Loss: 24.246349707245827, Learning rate: 0.0016\n",
      "Epoch 304/1100, Loss: 21.887292474508286, Learning rate: 0.0016\n",
      "Epoch 305/1100, Loss: 21.228975638747215, Learning rate: 0.0016\n",
      "Epoch 306/1100, Loss: 22.04928433895111, Learning rate: 0.0016\n",
      "Epoch 307/1100, Loss: 25.366086170077324, Learning rate: 0.0016\n",
      "Epoch 308/1100, Loss: 26.52703522145748, Learning rate: 0.0016\n",
      "Epoch 309/1100, Loss: 21.58054693043232, Learning rate: 0.0016\n",
      "Epoch 310/1100, Loss: 17.90289232134819, Learning rate: 0.0016\n",
      "Epoch 311/1100, Loss: 24.796317130327225, Learning rate: 0.0016\n",
      "Epoch 312/1100, Loss: 30.081832468509674, Learning rate: 0.0016\n",
      "Epoch 313/1100, Loss: 17.539926394820213, Learning rate: 0.0016\n",
      "Epoch 314/1100, Loss: 19.725359708070755, Learning rate: 0.0016\n",
      "Epoch 315/1100, Loss: 31.791841432452202, Learning rate: 0.0016\n",
      "Epoch 316/1100, Loss: 24.768191143870354, Learning rate: 0.0016\n",
      "Epoch 317/1100, Loss: 16.239636316895485, Learning rate: 0.0016\n",
      "Epoch 318/1100, Loss: 20.971083864569664, Learning rate: 0.0016\n",
      "Epoch 319/1100, Loss: 28.683246232569218, Learning rate: 0.0016\n",
      "Epoch 320/1100, Loss: 15.370373368263245, Learning rate: 0.0016\n",
      "Epoch 321/1100, Loss: 18.84912647306919, Learning rate: 0.0016\n",
      "Epoch 322/1100, Loss: 36.83863699436188, Learning rate: 0.0016\n",
      "Epoch 323/1100, Loss: 21.4700595587492, Learning rate: 0.0016\n",
      "Epoch 324/1100, Loss: 17.14171565324068, Learning rate: 0.0016\n",
      "Epoch 325/1100, Loss: 24.906760819256306, Learning rate: 0.0016\n",
      "Epoch 326/1100, Loss: 30.884487956762314, Learning rate: 0.0016\n",
      "Epoch 327/1100, Loss: 14.312067285180092, Learning rate: 0.0016\n",
      "Epoch 328/1100, Loss: 20.53545768558979, Learning rate: 0.0016\n",
      "Epoch 329/1100, Loss: 32.34430231153965, Learning rate: 0.0016\n",
      "Epoch 330/1100, Loss: 18.83389960974455, Learning rate: 0.0016\n",
      "Epoch 331/1100, Loss: 20.313138261437416, Learning rate: 0.0016\n",
      "Epoch 332/1100, Loss: 25.768021911382675, Learning rate: 0.0016\n",
      "Epoch 333/1100, Loss: 28.610046923160553, Learning rate: 0.0016\n",
      "Epoch 334/1100, Loss: 14.819703489542007, Learning rate: 0.0016\n",
      "Epoch 335/1100, Loss: 19.340072885155678, Learning rate: 0.0016\n",
      "Epoch 336/1100, Loss: 33.430465154349804, Learning rate: 0.0016\n",
      "Epoch 337/1100, Loss: 14.167976841330528, Learning rate: 0.0016\n",
      "Epoch 338/1100, Loss: 16.216444060206413, Learning rate: 0.0016\n",
      "Epoch 339/1100, Loss: 25.76672702282667, Learning rate: 0.0016\n",
      "Epoch 340/1100, Loss: 23.538636192679405, Learning rate: 0.0016\n",
      "Epoch 341/1100, Loss: 14.35901965200901, Learning rate: 0.0016\n",
      "Epoch 342/1100, Loss: 17.233711540699005, Learning rate: 0.0016\n",
      "Epoch 343/1100, Loss: 33.2009294629097, Learning rate: 0.0016\n",
      "Epoch 344/1100, Loss: 14.274003975093365, Learning rate: 0.0016\n",
      "Epoch 345/1100, Loss: 13.23128266632557, Learning rate: 0.00064\n",
      "Epoch 346/1100, Loss: 13.574103191494942, Learning rate: 0.00064\n",
      "Epoch 347/1100, Loss: 11.82804456166923, Learning rate: 0.00064\n",
      "Epoch 348/1100, Loss: 11.860916696488857, Learning rate: 0.00064\n",
      "Epoch 349/1100, Loss: 11.560004323720932, Learning rate: 0.00064\n",
      "Epoch 350/1100, Loss: 11.539312556385994, Learning rate: 0.00064\n",
      "Epoch 351/1100, Loss: 11.460783526301384, Learning rate: 0.00064\n",
      "Epoch 352/1100, Loss: 11.387963831424713, Learning rate: 0.00064\n",
      "Epoch 353/1100, Loss: 11.334033727645874, Learning rate: 0.00064\n",
      "Epoch 354/1100, Loss: 11.271386817097664, Learning rate: 0.00064\n",
      "Epoch 355/1100, Loss: 11.21597021073103, Learning rate: 0.00064\n",
      "Epoch 356/1100, Loss: 11.159382723271847, Learning rate: 0.00064\n",
      "Epoch 357/1100, Loss: 11.067439645528793, Learning rate: 0.00064\n",
      "Epoch 358/1100, Loss: 11.081896536052227, Learning rate: 0.00064\n",
      "Epoch 359/1100, Loss: 11.002050206065178, Learning rate: 0.00064\n",
      "Epoch 360/1100, Loss: 10.95630144327879, Learning rate: 0.00064\n",
      "Epoch 361/1100, Loss: 10.858879953622818, Learning rate: 0.00064\n",
      "Epoch 362/1100, Loss: 10.880014717578888, Learning rate: 0.00064\n",
      "Epoch 363/1100, Loss: 10.800596326589584, Learning rate: 0.00064\n",
      "Epoch 364/1100, Loss: 10.75599418580532, Learning rate: 0.00064\n",
      "Epoch 365/1100, Loss: 10.663513086736202, Learning rate: 0.00064\n",
      "Epoch 366/1100, Loss: 10.682696506381035, Learning rate: 0.00064\n",
      "Epoch 367/1100, Loss: 10.613491632044315, Learning rate: 0.00064\n",
      "Epoch 368/1100, Loss: 10.526816986501217, Learning rate: 0.00064\n",
      "Epoch 369/1100, Loss: 10.562762342393398, Learning rate: 0.00064\n",
      "Epoch 370/1100, Loss: 10.392808616161346, Learning rate: 0.00064\n",
      "Epoch 371/1100, Loss: 10.549996480345726, Learning rate: 0.00064\n",
      "Epoch 372/1100, Loss: 10.410565074533224, Learning rate: 0.00064\n",
      "Epoch 373/1100, Loss: 10.292314641177654, Learning rate: 0.00064\n",
      "Epoch 374/1100, Loss: 10.401334717869759, Learning rate: 0.00064\n",
      "Epoch 375/1100, Loss: 10.246044248342514, Learning rate: 0.00064\n",
      "Epoch 376/1100, Loss: 10.166102655231953, Learning rate: 0.00064\n",
      "Epoch 377/1100, Loss: 10.343414586037397, Learning rate: 0.00064\n",
      "Epoch 378/1100, Loss: 10.020327728241682, Learning rate: 0.00064\n",
      "Epoch 379/1100, Loss: 10.273186311125755, Learning rate: 0.00064\n",
      "Epoch 380/1100, Loss: 10.026329085230827, Learning rate: 0.00064\n",
      "Epoch 381/1100, Loss: 9.975299011915922, Learning rate: 0.00064\n",
      "Epoch 382/1100, Loss: 10.11265530064702, Learning rate: 0.00064\n",
      "Epoch 383/1100, Loss: 9.940527386963367, Learning rate: 0.00064\n",
      "Epoch 384/1100, Loss: 9.821334414184093, Learning rate: 0.00064\n",
      "Epoch 385/1100, Loss: 10.00638348609209, Learning rate: 0.00064\n",
      "Epoch 386/1100, Loss: 9.769609197974205, Learning rate: 0.00064\n",
      "Epoch 387/1100, Loss: 9.76670789718628, Learning rate: 0.00064\n",
      "Epoch 388/1100, Loss: 9.810450371354818, Learning rate: 0.00064\n",
      "Epoch 389/1100, Loss: 9.688477575778961, Learning rate: 0.00064\n",
      "Epoch 390/1100, Loss: 9.712540112435818, Learning rate: 0.00064\n",
      "Epoch 391/1100, Loss: 9.621204946190119, Learning rate: 0.00064\n",
      "Epoch 392/1100, Loss: 9.562674909830093, Learning rate: 0.00064\n",
      "Epoch 393/1100, Loss: 9.65727984905243, Learning rate: 0.00064\n",
      "Epoch 394/1100, Loss: 9.537472017109394, Learning rate: 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/1100, Loss: 9.409606482833624, Learning rate: 0.00064\n",
      "Epoch 396/1100, Loss: 9.546683389693499, Learning rate: 0.00064\n",
      "Epoch 397/1100, Loss: 9.457525808364153, Learning rate: 0.00064\n",
      "Epoch 398/1100, Loss: 9.27803397551179, Learning rate: 0.00064\n",
      "Epoch 399/1100, Loss: 9.449800867587328, Learning rate: 0.00064\n",
      "Epoch 400/1100, Loss: 9.341386798769236, Learning rate: 0.00064\n",
      "Epoch 401/1100, Loss: 9.169853214174509, Learning rate: 0.00064\n",
      "Epoch 402/1100, Loss: 9.325959974899888, Learning rate: 0.00064\n",
      "Epoch 403/1100, Loss: 9.21273497492075, Learning rate: 0.00064\n",
      "Epoch 404/1100, Loss: 9.18013720586896, Learning rate: 0.00064\n",
      "Epoch 405/1100, Loss: 9.149011880159378, Learning rate: 0.00064\n",
      "Epoch 406/1100, Loss: 9.067510087043047, Learning rate: 0.00064\n",
      "Epoch 407/1100, Loss: 9.052808947861195, Learning rate: 0.00064\n",
      "Epoch 408/1100, Loss: 9.083195999264717, Learning rate: 0.00064\n",
      "Epoch 409/1100, Loss: 9.073563637211919, Learning rate: 0.00064\n",
      "Epoch 410/1100, Loss: 8.855469889938831, Learning rate: 0.00064\n",
      "Epoch 411/1100, Loss: 9.02285024896264, Learning rate: 0.00064\n",
      "Epoch 412/1100, Loss: 8.996269594877958, Learning rate: 0.00064\n",
      "Epoch 413/1100, Loss: 8.718054562807083, Learning rate: 0.00064\n",
      "Epoch 414/1100, Loss: 8.95609201863408, Learning rate: 0.00064\n",
      "Epoch 415/1100, Loss: 8.825061563402414, Learning rate: 0.00064\n",
      "Epoch 416/1100, Loss: 8.853130120784044, Learning rate: 0.00064\n",
      "Epoch 417/1100, Loss: 8.69488101452589, Learning rate: 0.00064\n",
      "Epoch 418/1100, Loss: 8.763824371621013, Learning rate: 0.00064\n",
      "Epoch 419/1100, Loss: 8.59333311766386, Learning rate: 0.00064\n",
      "Epoch 420/1100, Loss: 8.923057230189443, Learning rate: 0.00064\n",
      "Epoch 421/1100, Loss: 8.263145599514246, Learning rate: 0.00064\n",
      "Epoch 422/1100, Loss: 8.93318053893745, Learning rate: 0.00064\n",
      "Epoch 423/1100, Loss: 8.774001188576221, Learning rate: 0.00064\n",
      "Epoch 424/1100, Loss: 8.284308768808842, Learning rate: 0.00064\n",
      "Epoch 425/1100, Loss: 9.101172247901559, Learning rate: 0.00064\n",
      "Epoch 426/1100, Loss: 8.172813683748245, Learning rate: 0.00064\n",
      "Epoch 427/1100, Loss: 8.507244989275932, Learning rate: 0.00064\n",
      "Epoch 428/1100, Loss: 8.922703568823636, Learning rate: 0.00064\n",
      "Epoch 429/1100, Loss: 7.990347009152174, Learning rate: 0.00064\n",
      "Epoch 430/1100, Loss: 8.438098415732384, Learning rate: 0.00064\n",
      "Epoch 431/1100, Loss: 9.161523656919599, Learning rate: 0.00064\n",
      "Epoch 432/1100, Loss: 7.873602733016014, Learning rate: 0.00064\n",
      "Epoch 433/1100, Loss: 8.509600553661585, Learning rate: 0.00064\n",
      "Epoch 434/1100, Loss: 9.247199013829231, Learning rate: 0.00064\n",
      "Epoch 435/1100, Loss: 7.8289182633161545, Learning rate: 0.00064\n",
      "Epoch 436/1100, Loss: 8.436966106295586, Learning rate: 0.00064\n",
      "Epoch 437/1100, Loss: 9.137483766302466, Learning rate: 0.00064\n",
      "Epoch 438/1100, Loss: 7.733729507774115, Learning rate: 0.00064\n",
      "Epoch 439/1100, Loss: 8.304818775504827, Learning rate: 0.00064\n",
      "Epoch 440/1100, Loss: 9.060826495289803, Learning rate: 0.00064\n",
      "Epoch 441/1100, Loss: 7.719142410904169, Learning rate: 0.00064\n",
      "Epoch 442/1100, Loss: 8.128562167286873, Learning rate: 0.00064\n",
      "Epoch 443/1100, Loss: 8.944493245333433, Learning rate: 0.00064\n",
      "Epoch 444/1100, Loss: 7.80696078017354, Learning rate: 0.00064\n",
      "Epoch 445/1100, Loss: 7.984907582402229, Learning rate: 0.00064\n",
      "Epoch 446/1100, Loss: 8.581876195035875, Learning rate: 0.00064\n",
      "Epoch 447/1100, Loss: 7.704542173072696, Learning rate: 0.00064\n",
      "Epoch 448/1100, Loss: 7.823036342859268, Learning rate: 0.00064\n",
      "Epoch 449/1100, Loss: 8.175125235691667, Learning rate: 0.00064\n",
      "Epoch 450/1100, Loss: 7.760844312608242, Learning rate: 0.00064\n",
      "Epoch 451/1100, Loss: 7.605163849890232, Learning rate: 0.00064\n",
      "Epoch 452/1100, Loss: 8.10207443125546, Learning rate: 0.00064\n",
      "Epoch 453/1100, Loss: 7.680968139320612, Learning rate: 0.00064\n",
      "Epoch 454/1100, Loss: 7.5169205367565155, Learning rate: 0.00064\n",
      "Epoch 455/1100, Loss: 8.04010888747871, Learning rate: 0.00064\n",
      "Epoch 456/1100, Loss: 7.63430543243885, Learning rate: 0.00064\n",
      "Epoch 457/1100, Loss: 7.419055663049221, Learning rate: 0.00064\n",
      "Epoch 458/1100, Loss: 8.00022616237402, Learning rate: 0.00064\n",
      "Epoch 459/1100, Loss: 7.6184744220227, Learning rate: 0.00064\n",
      "Epoch 460/1100, Loss: 7.4159651175141335, Learning rate: 0.00025600000000000004\n",
      "Epoch 461/1100, Loss: 6.749386226758361, Learning rate: 0.00025600000000000004\n",
      "Epoch 462/1100, Loss: 6.698219083249569, Learning rate: 0.00025600000000000004\n",
      "Epoch 463/1100, Loss: 6.647444946691394, Learning rate: 0.00025600000000000004\n",
      "Epoch 464/1100, Loss: 6.597759731113911, Learning rate: 0.00025600000000000004\n",
      "Epoch 465/1100, Loss: 6.63726469501853, Learning rate: 0.00025600000000000004\n",
      "Epoch 466/1100, Loss: 6.581893619149923, Learning rate: 0.00025600000000000004\n",
      "Epoch 467/1100, Loss: 6.677679423242807, Learning rate: 0.00025600000000000004\n",
      "Epoch 468/1100, Loss: 6.569727774709463, Learning rate: 0.00025600000000000004\n",
      "Epoch 469/1100, Loss: 6.651554990559816, Learning rate: 0.00025600000000000004\n",
      "Epoch 470/1100, Loss: 6.645788049325347, Learning rate: 0.00025600000000000004\n",
      "Epoch 471/1100, Loss: 6.528001852333546, Learning rate: 0.00025600000000000004\n",
      "Epoch 472/1100, Loss: 6.639105837792158, Learning rate: 0.00025600000000000004\n",
      "Epoch 473/1100, Loss: 6.519820073619485, Learning rate: 0.00025600000000000004\n",
      "Epoch 474/1100, Loss: 6.554117862135172, Learning rate: 0.00025600000000000004\n",
      "Epoch 475/1100, Loss: 6.678792109712958, Learning rate: 0.00025600000000000004\n",
      "Epoch 476/1100, Loss: 6.497385647147894, Learning rate: 0.00025600000000000004\n",
      "Epoch 477/1100, Loss: 6.569833377376199, Learning rate: 0.00025600000000000004\n",
      "Epoch 478/1100, Loss: 6.551867825910449, Learning rate: 0.00025600000000000004\n",
      "Epoch 479/1100, Loss: 6.483110181987286, Learning rate: 0.00025600000000000004\n",
      "Epoch 480/1100, Loss: 6.621731728315353, Learning rate: 0.00025600000000000004\n",
      "Epoch 481/1100, Loss: 6.433973256498575, Learning rate: 0.00025600000000000004\n",
      "Epoch 482/1100, Loss: 6.527414310723543, Learning rate: 0.00025600000000000004\n",
      "Epoch 483/1100, Loss: 6.4288071151822805, Learning rate: 0.00025600000000000004\n",
      "Epoch 484/1100, Loss: 6.600981285795569, Learning rate: 0.00025600000000000004\n",
      "Epoch 485/1100, Loss: 6.410571079701185, Learning rate: 0.00025600000000000004\n",
      "Epoch 486/1100, Loss: 6.463403904810548, Learning rate: 0.00025600000000000004\n",
      "Epoch 487/1100, Loss: 6.371357180178165, Learning rate: 0.00025600000000000004\n",
      "Epoch 488/1100, Loss: 6.442845061421394, Learning rate: 0.00025600000000000004\n",
      "Epoch 489/1100, Loss: 6.491740122437477, Learning rate: 0.00025600000000000004\n",
      "Epoch 490/1100, Loss: 6.33123279735446, Learning rate: 0.00025600000000000004\n",
      "Epoch 491/1100, Loss: 6.392487278208137, Learning rate: 0.00025600000000000004\n",
      "Epoch 492/1100, Loss: 6.327159931883216, Learning rate: 0.00025600000000000004\n",
      "Epoch 493/1100, Loss: 6.336006911471486, Learning rate: 0.00025600000000000004\n",
      "Epoch 494/1100, Loss: 6.428824003785849, Learning rate: 0.00025600000000000004\n",
      "Epoch 495/1100, Loss: 6.412116043269634, Learning rate: 0.00025600000000000004\n",
      "Epoch 496/1100, Loss: 6.313958004117012, Learning rate: 0.00025600000000000004\n",
      "Epoch 497/1100, Loss: 6.377369467169046, Learning rate: 0.00025600000000000004\n",
      "Epoch 498/1100, Loss: 6.283915631473064, Learning rate: 0.00025600000000000004\n",
      "Epoch 499/1100, Loss: 6.2939043920487165, Learning rate: 0.00025600000000000004\n",
      "Epoch 500/1100, Loss: 6.3823971170932055, Learning rate: 0.00025600000000000004\n",
      "Epoch 501/1100, Loss: 6.2726226802915335, Learning rate: 0.00025600000000000004\n",
      "Epoch 502/1100, Loss: 6.32634543068707, Learning rate: 0.00025600000000000004\n",
      "Epoch 503/1100, Loss: 6.331619093194604, Learning rate: 0.00025600000000000004\n",
      "Epoch 504/1100, Loss: 6.243104191496968, Learning rate: 0.00025600000000000004\n",
      "Epoch 505/1100, Loss: 6.255033893510699, Learning rate: 0.00025600000000000004\n",
      "Epoch 506/1100, Loss: 6.355151938274503, Learning rate: 0.00025600000000000004\n",
      "Epoch 507/1100, Loss: 6.2101832665503025, Learning rate: 0.00025600000000000004\n",
      "Epoch 508/1100, Loss: 6.238210642710328, Learning rate: 0.00025600000000000004\n",
      "Epoch 509/1100, Loss: 6.300849296152592, Learning rate: 0.00025600000000000004\n",
      "Epoch 510/1100, Loss: 6.188073569908738, Learning rate: 0.00025600000000000004\n",
      "Epoch 511/1100, Loss: 6.244905088096857, Learning rate: 0.00025600000000000004\n",
      "Epoch 512/1100, Loss: 6.285566465929151, Learning rate: 0.00025600000000000004\n",
      "Epoch 513/1100, Loss: 6.125614583492279, Learning rate: 0.00025600000000000004\n",
      "Epoch 514/1100, Loss: 6.218374323099852, Learning rate: 0.00025600000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 515/1100, Loss: 6.277864160016179, Learning rate: 0.00025600000000000004\n",
      "Epoch 516/1100, Loss: 6.111568436026573, Learning rate: 0.00025600000000000004\n",
      "Epoch 517/1100, Loss: 6.153274845331907, Learning rate: 0.00025600000000000004\n",
      "Epoch 518/1100, Loss: 6.329136189073324, Learning rate: 0.00025600000000000004\n",
      "Epoch 519/1100, Loss: 6.051200956106186, Learning rate: 0.00025600000000000004\n",
      "Epoch 520/1100, Loss: 6.225703718140721, Learning rate: 0.00025600000000000004\n",
      "Epoch 521/1100, Loss: 6.103267252445221, Learning rate: 0.00025600000000000004\n",
      "Epoch 522/1100, Loss: 6.2313648872077465, Learning rate: 0.00025600000000000004\n",
      "Epoch 523/1100, Loss: 6.048612266778946, Learning rate: 0.00025600000000000004\n",
      "Epoch 524/1100, Loss: 6.209988716989756, Learning rate: 0.00025600000000000004\n",
      "Epoch 525/1100, Loss: 6.0403742007911205, Learning rate: 0.00025600000000000004\n",
      "Epoch 526/1100, Loss: 6.122510526329279, Learning rate: 0.00025600000000000004\n",
      "Epoch 527/1100, Loss: 6.184899611398578, Learning rate: 0.00025600000000000004\n",
      "Epoch 528/1100, Loss: 5.973836287856102, Learning rate: 0.00025600000000000004\n",
      "Epoch 529/1100, Loss: 6.124817822128534, Learning rate: 0.00025600000000000004\n",
      "Epoch 530/1100, Loss: 6.142682205885649, Learning rate: 0.00025600000000000004\n",
      "Epoch 531/1100, Loss: 6.025892397388816, Learning rate: 0.00025600000000000004\n",
      "Epoch 532/1100, Loss: 6.021401733160019, Learning rate: 0.00025600000000000004\n",
      "Epoch 533/1100, Loss: 6.13265360891819, Learning rate: 0.00025600000000000004\n",
      "Epoch 534/1100, Loss: 5.932369619607925, Learning rate: 0.00025600000000000004\n",
      "Epoch 535/1100, Loss: 6.0391942616552114, Learning rate: 0.00025600000000000004\n",
      "Epoch 536/1100, Loss: 6.1369242034852505, Learning rate: 0.00025600000000000004\n",
      "Epoch 537/1100, Loss: 5.862482700496912, Learning rate: 0.00025600000000000004\n",
      "Epoch 538/1100, Loss: 6.062171712517738, Learning rate: 0.00025600000000000004\n",
      "Epoch 539/1100, Loss: 6.100238285958767, Learning rate: 0.00025600000000000004\n",
      "Epoch 540/1100, Loss: 5.872896756976843, Learning rate: 0.00025600000000000004\n",
      "Epoch 541/1100, Loss: 6.008183348923922, Learning rate: 0.00025600000000000004\n",
      "Epoch 542/1100, Loss: 5.887092474848032, Learning rate: 0.00025600000000000004\n",
      "Epoch 543/1100, Loss: 6.12623393908143, Learning rate: 0.00025600000000000004\n",
      "Epoch 544/1100, Loss: 5.819026464596391, Learning rate: 0.00025600000000000004\n",
      "Epoch 545/1100, Loss: 6.001134939491749, Learning rate: 0.00025600000000000004\n",
      "Epoch 546/1100, Loss: 5.906865566968918, Learning rate: 0.00025600000000000004\n",
      "Epoch 547/1100, Loss: 5.834480127319694, Learning rate: 0.00025600000000000004\n",
      "Epoch 548/1100, Loss: 6.099081184715033, Learning rate: 0.00025600000000000004\n",
      "Epoch 549/1100, Loss: 5.777760162949562, Learning rate: 0.00025600000000000004\n",
      "Epoch 550/1100, Loss: 5.959440179169178, Learning rate: 0.00025600000000000004\n",
      "Epoch 551/1100, Loss: 5.801130145788193, Learning rate: 0.00025600000000000004\n",
      "Epoch 552/1100, Loss: 5.935870258137584, Learning rate: 0.00025600000000000004\n",
      "Epoch 553/1100, Loss: 5.800638189539313, Learning rate: 0.00025600000000000004\n",
      "Epoch 554/1100, Loss: 5.860475420951843, Learning rate: 0.00025600000000000004\n",
      "Epoch 555/1100, Loss: 5.808173686265945, Learning rate: 0.00025600000000000004\n",
      "Epoch 556/1100, Loss: 5.802031321451068, Learning rate: 0.00025600000000000004\n",
      "Epoch 557/1100, Loss: 5.935670861974359, Learning rate: 0.00025600000000000004\n",
      "Epoch 558/1100, Loss: 5.7965437322855, Learning rate: 0.00025600000000000004\n",
      "Epoch 559/1100, Loss: 5.794263988733292, Learning rate: 0.00025600000000000004\n",
      "Epoch 560/1100, Loss: 5.9337911028414965, Learning rate: 0.00025600000000000004\n",
      "Epoch 561/1100, Loss: 5.676908031105995, Learning rate: 0.00025600000000000004\n",
      "Epoch 562/1100, Loss: 5.919450389221311, Learning rate: 0.00025600000000000004\n",
      "Epoch 563/1100, Loss: 5.962120587006211, Learning rate: 0.00025600000000000004\n",
      "Epoch 564/1100, Loss: 5.669524569064379, Learning rate: 0.00025600000000000004\n",
      "Epoch 565/1100, Loss: 5.828298840671778, Learning rate: 0.00025600000000000004\n",
      "Epoch 566/1100, Loss: 5.909769218415022, Learning rate: 0.00025600000000000004\n",
      "Epoch 567/1100, Loss: 5.6546290051192045, Learning rate: 0.00025600000000000004\n",
      "Epoch 568/1100, Loss: 5.791010692715645, Learning rate: 0.00025600000000000004\n",
      "Epoch 569/1100, Loss: 5.8429394364356995, Learning rate: 0.00025600000000000004\n",
      "Epoch 570/1100, Loss: 5.614549206569791, Learning rate: 0.00025600000000000004\n",
      "Epoch 571/1100, Loss: 5.792313389480114, Learning rate: 0.00025600000000000004\n",
      "Epoch 572/1100, Loss: 5.965134143829346, Learning rate: 0.00025600000000000004\n",
      "Epoch 573/1100, Loss: 5.58390885591507, Learning rate: 0.00025600000000000004\n",
      "Epoch 574/1100, Loss: 5.732573660090566, Learning rate: 0.00025600000000000004\n",
      "Epoch 575/1100, Loss: 5.881773727014661, Learning rate: 0.00010240000000000002\n",
      "Epoch 576/1100, Loss: 5.364551834762096, Learning rate: 0.00010240000000000002\n",
      "Epoch 577/1100, Loss: 5.378898065537214, Learning rate: 0.00010240000000000002\n",
      "Epoch 578/1100, Loss: 5.362828195095062, Learning rate: 0.00010240000000000002\n",
      "Epoch 579/1100, Loss: 5.347026236355305, Learning rate: 0.00010240000000000002\n",
      "Epoch 580/1100, Loss: 5.3485128954052925, Learning rate: 0.00010240000000000002\n",
      "Epoch 581/1100, Loss: 5.358412619680166, Learning rate: 0.00010240000000000002\n",
      "Epoch 582/1100, Loss: 5.3425925597548485, Learning rate: 0.00010240000000000002\n",
      "Epoch 583/1100, Loss: 5.34347845800221, Learning rate: 0.00010240000000000002\n",
      "Epoch 584/1100, Loss: 5.335211295634508, Learning rate: 0.00010240000000000002\n",
      "Epoch 585/1100, Loss: 5.337386192753911, Learning rate: 0.00010240000000000002\n",
      "Epoch 586/1100, Loss: 5.328803479671478, Learning rate: 0.00010240000000000002\n",
      "Epoch 587/1100, Loss: 5.321147745475173, Learning rate: 0.00010240000000000002\n",
      "Epoch 588/1100, Loss: 5.323529168963432, Learning rate: 0.00010240000000000002\n",
      "Epoch 589/1100, Loss: 5.329934660345316, Learning rate: 0.00010240000000000002\n",
      "Epoch 590/1100, Loss: 5.332504643127322, Learning rate: 0.00010240000000000002\n",
      "Epoch 591/1100, Loss: 5.318249877542257, Learning rate: 0.00010240000000000002\n",
      "Epoch 592/1100, Loss: 5.360945004969835, Learning rate: 0.00010240000000000002\n",
      "Epoch 593/1100, Loss: 5.330039948225021, Learning rate: 0.00010240000000000002\n",
      "Epoch 594/1100, Loss: 5.3233923595398664, Learning rate: 0.00010240000000000002\n",
      "Epoch 595/1100, Loss: 5.325575832277536, Learning rate: 0.00010240000000000002\n",
      "Epoch 596/1100, Loss: 5.295632336288691, Learning rate: 0.00010240000000000002\n",
      "Epoch 597/1100, Loss: 5.302844241261482, Learning rate: 0.00010240000000000002\n",
      "Epoch 598/1100, Loss: 5.301141524687409, Learning rate: 0.00010240000000000002\n",
      "Epoch 599/1100, Loss: 5.296778354793787, Learning rate: 0.00010240000000000002\n",
      "Epoch 600/1100, Loss: 5.293938767164946, Learning rate: 0.00010240000000000002\n",
      "Epoch 601/1100, Loss: 5.287995846942067, Learning rate: 0.00010240000000000002\n",
      "Epoch 602/1100, Loss: 5.302521035075188, Learning rate: 0.00010240000000000002\n",
      "Epoch 603/1100, Loss: 5.304398499429226, Learning rate: 0.00010240000000000002\n",
      "Epoch 604/1100, Loss: 5.28872050344944, Learning rate: 0.00010240000000000002\n",
      "Epoch 605/1100, Loss: 5.287031704559922, Learning rate: 0.00010240000000000002\n",
      "Epoch 606/1100, Loss: 5.2796664871275425, Learning rate: 0.00010240000000000002\n",
      "Epoch 607/1100, Loss: 5.28954353928566, Learning rate: 0.00010240000000000002\n",
      "Epoch 608/1100, Loss: 5.279935775324702, Learning rate: 0.00010240000000000002\n",
      "Epoch 609/1100, Loss: 5.2553855665028095, Learning rate: 0.00010240000000000002\n",
      "Epoch 610/1100, Loss: 5.265579482540488, Learning rate: 0.00010240000000000002\n",
      "Epoch 611/1100, Loss: 5.264093579724431, Learning rate: 0.00010240000000000002\n",
      "Epoch 612/1100, Loss: 5.282325046136975, Learning rate: 0.00010240000000000002\n",
      "Epoch 613/1100, Loss: 5.26943876221776, Learning rate: 0.00010240000000000002\n",
      "Epoch 614/1100, Loss: 5.288752164691687, Learning rate: 0.00010240000000000002\n",
      "Epoch 615/1100, Loss: 5.26587625592947, Learning rate: 0.00010240000000000002\n",
      "Epoch 616/1100, Loss: 5.250098928809166, Learning rate: 0.00010240000000000002\n",
      "Epoch 617/1100, Loss: 5.255191942676902, Learning rate: 0.00010240000000000002\n",
      "Epoch 618/1100, Loss: 5.2330066952854395, Learning rate: 0.00010240000000000002\n",
      "Epoch 619/1100, Loss: 5.24567973613739, Learning rate: 0.00010240000000000002\n",
      "Epoch 620/1100, Loss: 5.238342400640249, Learning rate: 0.00010240000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 621/1100, Loss: 5.293504603207111, Learning rate: 0.00010240000000000002\n",
      "Epoch 622/1100, Loss: 5.241427607834339, Learning rate: 0.00010240000000000002\n",
      "Epoch 623/1100, Loss: 5.252404663711786, Learning rate: 0.00010240000000000002\n",
      "Epoch 624/1100, Loss: 5.246334798634052, Learning rate: 0.00010240000000000002\n",
      "Epoch 625/1100, Loss: 5.212492575868964, Learning rate: 0.00010240000000000002\n",
      "Epoch 626/1100, Loss: 5.219064956530929, Learning rate: 0.00010240000000000002\n",
      "Epoch 627/1100, Loss: 5.2216117437928915, Learning rate: 0.00010240000000000002\n",
      "Epoch 628/1100, Loss: 5.212666191160679, Learning rate: 0.00010240000000000002\n",
      "Epoch 629/1100, Loss: 5.2091250363737345, Learning rate: 0.00010240000000000002\n",
      "Epoch 630/1100, Loss: 5.207488624379039, Learning rate: 0.00010240000000000002\n",
      "Epoch 631/1100, Loss: 5.2688173577189445, Learning rate: 0.00010240000000000002\n",
      "Epoch 632/1100, Loss: 5.214907320216298, Learning rate: 0.00010240000000000002\n",
      "Epoch 633/1100, Loss: 5.219300566241145, Learning rate: 0.00010240000000000002\n",
      "Epoch 634/1100, Loss: 5.214392634108663, Learning rate: 0.00010240000000000002\n",
      "Epoch 635/1100, Loss: 5.178013939410448, Learning rate: 0.00010240000000000002\n",
      "Epoch 636/1100, Loss: 5.1910842545330524, Learning rate: 0.00010240000000000002\n",
      "Epoch 637/1100, Loss: 5.1900179497897625, Learning rate: 0.00010240000000000002\n",
      "Epoch 638/1100, Loss: 5.180135460570455, Learning rate: 0.00010240000000000002\n",
      "Epoch 639/1100, Loss: 5.182930959388614, Learning rate: 0.00010240000000000002\n",
      "Epoch 640/1100, Loss: 5.1954220831394196, Learning rate: 0.00010240000000000002\n",
      "Epoch 641/1100, Loss: 5.178968185558915, Learning rate: 0.00010240000000000002\n",
      "Epoch 642/1100, Loss: 5.209281586110592, Learning rate: 0.00010240000000000002\n",
      "Epoch 643/1100, Loss: 5.184410849586129, Learning rate: 0.00010240000000000002\n",
      "Epoch 644/1100, Loss: 5.155561991035938, Learning rate: 0.00010240000000000002\n",
      "Epoch 645/1100, Loss: 5.191452154889703, Learning rate: 0.00010240000000000002\n",
      "Epoch 646/1100, Loss: 5.1565316412597895, Learning rate: 0.00010240000000000002\n",
      "Epoch 647/1100, Loss: 5.165581997483969, Learning rate: 0.00010240000000000002\n",
      "Epoch 648/1100, Loss: 5.165978213772178, Learning rate: 0.00010240000000000002\n",
      "Epoch 649/1100, Loss: 5.157867964357138, Learning rate: 0.00010240000000000002\n",
      "Epoch 650/1100, Loss: 5.1444497890770435, Learning rate: 0.00010240000000000002\n",
      "Epoch 651/1100, Loss: 5.177863070741296, Learning rate: 0.00010240000000000002\n",
      "Epoch 652/1100, Loss: 5.167090190574527, Learning rate: 0.00010240000000000002\n",
      "Epoch 653/1100, Loss: 5.1479143016040325, Learning rate: 0.00010240000000000002\n",
      "Epoch 654/1100, Loss: 5.132612669840455, Learning rate: 0.00010240000000000002\n",
      "Epoch 655/1100, Loss: 5.150868579745293, Learning rate: 0.00010240000000000002\n",
      "Epoch 656/1100, Loss: 5.179164228960872, Learning rate: 0.00010240000000000002\n",
      "Epoch 657/1100, Loss: 5.151656793430448, Learning rate: 0.00010240000000000002\n",
      "Epoch 658/1100, Loss: 5.116154303774238, Learning rate: 0.00010240000000000002\n",
      "Epoch 659/1100, Loss: 5.1471932996064425, Learning rate: 0.00010240000000000002\n",
      "Epoch 660/1100, Loss: 5.124334026128054, Learning rate: 0.00010240000000000002\n",
      "Epoch 661/1100, Loss: 5.138707723468542, Learning rate: 0.00010240000000000002\n",
      "Epoch 662/1100, Loss: 5.105167534202337, Learning rate: 0.00010240000000000002\n",
      "Epoch 663/1100, Loss: 5.109916096553206, Learning rate: 0.00010240000000000002\n",
      "Epoch 664/1100, Loss: 5.107890667393804, Learning rate: 0.00010240000000000002\n",
      "Epoch 665/1100, Loss: 5.102386128157377, Learning rate: 0.00010240000000000002\n",
      "Epoch 666/1100, Loss: 5.127607893198729, Learning rate: 0.00010240000000000002\n",
      "Epoch 667/1100, Loss: 5.106542395427823, Learning rate: 0.00010240000000000002\n",
      "Epoch 668/1100, Loss: 5.133398432284594, Learning rate: 0.00010240000000000002\n",
      "Epoch 669/1100, Loss: 5.106190036982298, Learning rate: 0.00010240000000000002\n",
      "Epoch 670/1100, Loss: 5.074761228635907, Learning rate: 0.00010240000000000002\n",
      "Epoch 671/1100, Loss: 5.125833168625832, Learning rate: 0.00010240000000000002\n",
      "Epoch 672/1100, Loss: 5.081815762445331, Learning rate: 0.00010240000000000002\n",
      "Epoch 673/1100, Loss: 5.088471509516239, Learning rate: 0.00010240000000000002\n",
      "Epoch 674/1100, Loss: 5.083733160048723, Learning rate: 0.00010240000000000002\n",
      "Epoch 675/1100, Loss: 5.077961636707187, Learning rate: 0.00010240000000000002\n",
      "Epoch 676/1100, Loss: 5.073904858902097, Learning rate: 0.00010240000000000002\n",
      "Epoch 677/1100, Loss: 5.109314540401101, Learning rate: 0.00010240000000000002\n",
      "Epoch 678/1100, Loss: 5.082817951217294, Learning rate: 0.00010240000000000002\n",
      "Epoch 679/1100, Loss: 5.060623019933701, Learning rate: 0.00010240000000000002\n",
      "Epoch 680/1100, Loss: 5.066379889845848, Learning rate: 0.00010240000000000002\n",
      "Epoch 681/1100, Loss: 5.050607008859515, Learning rate: 0.00010240000000000002\n",
      "Epoch 682/1100, Loss: 5.055162459611893, Learning rate: 0.00010240000000000002\n",
      "Epoch 683/1100, Loss: 5.052763558924198, Learning rate: 0.00010240000000000002\n",
      "Epoch 684/1100, Loss: 5.07145812176168, Learning rate: 0.00010240000000000002\n",
      "Epoch 685/1100, Loss: 5.052547862753272, Learning rate: 0.00010240000000000002\n",
      "Epoch 686/1100, Loss: 5.066906206309795, Learning rate: 0.00010240000000000002\n",
      "Epoch 687/1100, Loss: 5.033410217612982, Learning rate: 0.00010240000000000002\n",
      "Epoch 688/1100, Loss: 5.044464312493801, Learning rate: 0.00010240000000000002\n",
      "Epoch 689/1100, Loss: 5.038753554224968, Learning rate: 0.00010240000000000002\n",
      "Epoch 690/1100, Loss: 5.027720352634788, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 691/1100, Loss: 4.913427297025919, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 692/1100, Loss: 4.9200685899704695, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 693/1100, Loss: 4.90909562446177, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 694/1100, Loss: 4.908292567357421, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 695/1100, Loss: 4.906157432124019, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 696/1100, Loss: 4.90595394000411, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 697/1100, Loss: 4.9033684227615595, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 698/1100, Loss: 4.899997152388096, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 699/1100, Loss: 4.902053192257881, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 700/1100, Loss: 4.89864968508482, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 701/1100, Loss: 4.902819115668535, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 702/1100, Loss: 4.901930510997772, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 703/1100, Loss: 4.903387393802404, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 704/1100, Loss: 4.893136449158192, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 705/1100, Loss: 4.897845393046737, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 706/1100, Loss: 4.894949363544583, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 707/1100, Loss: 4.893880918622017, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 708/1100, Loss: 4.895329486578703, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 709/1100, Loss: 4.887100260704756, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 710/1100, Loss: 4.887136798352003, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 711/1100, Loss: 4.892710069194436, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 712/1100, Loss: 4.891067214310169, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 713/1100, Loss: 4.887446608394384, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 714/1100, Loss: 4.9039468094706535, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 715/1100, Loss: 4.874314831569791, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 716/1100, Loss: 4.888874271884561, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 717/1100, Loss: 4.887478934600949, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 718/1100, Loss: 4.881169708445668, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 719/1100, Loss: 4.898909702897072, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 720/1100, Loss: 4.871037961915135, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 721/1100, Loss: 4.87899205647409, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 722/1100, Loss: 4.879260333254933, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 723/1100, Loss: 4.875911833718419, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 724/1100, Loss: 4.893414746969938, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 725/1100, Loss: 4.86929539963603, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 726/1100, Loss: 4.880111295729876, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 727/1100, Loss: 4.872235247865319, Learning rate: 4.0960000000000014e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 728/1100, Loss: 4.868560388684273, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 729/1100, Loss: 4.8668566737324, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 730/1100, Loss: 4.86826977878809, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 731/1100, Loss: 4.864958023652434, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 732/1100, Loss: 4.866473019123077, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 733/1100, Loss: 4.864749401807785, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 734/1100, Loss: 4.865449342876673, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 735/1100, Loss: 4.881811054423451, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 736/1100, Loss: 4.857517706230283, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 737/1100, Loss: 4.871251756325364, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 738/1100, Loss: 4.860784819349647, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 739/1100, Loss: 4.852283578366041, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 740/1100, Loss: 4.852904435247183, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 741/1100, Loss: 4.854127563536167, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 742/1100, Loss: 4.852455059066415, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 743/1100, Loss: 4.854467460885644, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 744/1100, Loss: 4.852946763858199, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 745/1100, Loss: 4.852675072848797, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 746/1100, Loss: 4.8704073671251535, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 747/1100, Loss: 4.844527827575803, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 748/1100, Loss: 4.855818118900061, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 749/1100, Loss: 4.84665990807116, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 750/1100, Loss: 4.840787824243307, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 751/1100, Loss: 4.84501632861793, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 752/1100, Loss: 4.842336382716894, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 753/1100, Loss: 4.835622601211071, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 754/1100, Loss: 4.8363367058336735, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 755/1100, Loss: 4.837147178128362, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 756/1100, Loss: 4.8363310638815165, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 757/1100, Loss: 4.8412943333387375, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 758/1100, Loss: 4.841619057580829, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 759/1100, Loss: 4.837666561827064, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 760/1100, Loss: 4.830355141311884, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 761/1100, Loss: 4.830538535490632, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 762/1100, Loss: 4.831508383154869, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 763/1100, Loss: 4.8301234524697065, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 764/1100, Loss: 4.832586200907826, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 765/1100, Loss: 4.847991943359375, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 766/1100, Loss: 4.823952419683337, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 767/1100, Loss: 4.838621793314815, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 768/1100, Loss: 4.826152715831995, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 769/1100, Loss: 4.816292403265834, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 770/1100, Loss: 4.8223894238471985, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 771/1100, Loss: 4.818952454254031, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 772/1100, Loss: 4.816909395158291, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 773/1100, Loss: 4.8165408577769995, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 774/1100, Loss: 4.81460896320641, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 775/1100, Loss: 4.814554162323475, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 776/1100, Loss: 4.819519434124231, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 777/1100, Loss: 4.817452970892191, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 778/1100, Loss: 4.814435951411724, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 779/1100, Loss: 4.8318743743002415, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 780/1100, Loss: 4.808005707338452, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 781/1100, Loss: 4.82051694393158, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 782/1100, Loss: 4.80675895139575, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 783/1100, Loss: 4.802185453474522, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 784/1100, Loss: 4.8032868057489395, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 785/1100, Loss: 4.802516082301736, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 786/1100, Loss: 4.800627635791898, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 787/1100, Loss: 4.800824264064431, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 788/1100, Loss: 4.8004294484853745, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 789/1100, Loss: 4.810839844867587, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 790/1100, Loss: 4.795600518584251, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 791/1100, Loss: 4.795224765315652, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 792/1100, Loss: 4.793511975556612, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 793/1100, Loss: 4.79657126404345, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 794/1100, Loss: 4.798933135345578, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 795/1100, Loss: 4.797037899494171, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 796/1100, Loss: 4.785611782222986, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 797/1100, Loss: 4.792142152786255, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 798/1100, Loss: 4.794145356863737, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 799/1100, Loss: 4.78735875338316, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 800/1100, Loss: 4.809884445741773, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 801/1100, Loss: 4.781965646892786, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 802/1100, Loss: 4.800534717738628, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 803/1100, Loss: 4.776921337470412, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 804/1100, Loss: 4.7870524022728205, Learning rate: 4.0960000000000014e-05\n",
      "Epoch 805/1100, Loss: 4.784235615283251, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 806/1100, Loss: 4.731350293383002, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 807/1100, Loss: 4.735338676720858, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 808/1100, Loss: 4.733246555551887, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 809/1100, Loss: 4.734709080308676, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 810/1100, Loss: 4.733538042753935, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 811/1100, Loss: 4.730357943102717, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 812/1100, Loss: 4.731074072420597, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 813/1100, Loss: 4.730624925345182, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 814/1100, Loss: 4.732029939070344, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 815/1100, Loss: 4.731183744966984, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 816/1100, Loss: 4.727750662714243, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 817/1100, Loss: 4.729029832407832, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 818/1100, Loss: 4.728909129276872, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 819/1100, Loss: 4.726461989805102, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 820/1100, Loss: 4.728753384202719, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 821/1100, Loss: 4.728085156530142, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 822/1100, Loss: 4.724699884653091, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 823/1100, Loss: 4.724356630817056, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 824/1100, Loss: 4.727179482579231, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 825/1100, Loss: 4.727816041558981, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 826/1100, Loss: 4.723553508520126, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 827/1100, Loss: 4.726648636162281, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 828/1100, Loss: 4.724087418988347, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 829/1100, Loss: 4.722188852727413, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 830/1100, Loss: 4.721995083615184, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 831/1100, Loss: 4.722248865291476, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 832/1100, Loss: 4.721257207915187, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 833/1100, Loss: 4.72203085757792, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 834/1100, Loss: 4.72192563675344, Learning rate: 1.6384000000000008e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/1100, Loss: 4.722715796902776, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 836/1100, Loss: 4.719285499304533, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 837/1100, Loss: 4.727842688560486, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 838/1100, Loss: 4.715662036091089, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 839/1100, Loss: 4.72061488032341, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 840/1100, Loss: 4.719684787094593, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 841/1100, Loss: 4.716357201337814, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 842/1100, Loss: 4.7158940769732, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 843/1100, Loss: 4.715787762776017, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 844/1100, Loss: 4.716280810534954, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 845/1100, Loss: 4.717180186882615, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 846/1100, Loss: 4.71727592498064, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 847/1100, Loss: 4.714650202542543, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 848/1100, Loss: 4.715661812573671, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 849/1100, Loss: 4.714127393439412, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 850/1100, Loss: 4.714974571019411, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 851/1100, Loss: 4.7120772413909435, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 852/1100, Loss: 4.711693709716201, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 853/1100, Loss: 4.7111586183309555, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 854/1100, Loss: 4.711189828813076, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 855/1100, Loss: 4.710587682202458, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 856/1100, Loss: 4.712870558723807, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 857/1100, Loss: 4.7129774410277605, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 858/1100, Loss: 4.709310375154018, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 859/1100, Loss: 4.710969716310501, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 860/1100, Loss: 4.708782818168402, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 861/1100, Loss: 4.70849715732038, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 862/1100, Loss: 4.70728256367147, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 863/1100, Loss: 4.708861885592341, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 864/1100, Loss: 4.708831589668989, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 865/1100, Loss: 4.70580462180078, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 866/1100, Loss: 4.70546854659915, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 867/1100, Loss: 4.704900227487087, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 868/1100, Loss: 4.707047605887055, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 869/1100, Loss: 4.705764686688781, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 870/1100, Loss: 4.713770331814885, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 871/1100, Loss: 4.701899446547031, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 872/1100, Loss: 4.706816341727972, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 873/1100, Loss: 4.704157644882798, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 874/1100, Loss: 4.70022820495069, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 875/1100, Loss: 4.702042944729328, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 876/1100, Loss: 4.701826855540276, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 877/1100, Loss: 4.700431689620018, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 878/1100, Loss: 4.69958577491343, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 879/1100, Loss: 4.699394499883056, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 880/1100, Loss: 4.701966974884272, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 881/1100, Loss: 4.702011970803142, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 882/1100, Loss: 4.697922186926007, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 883/1100, Loss: 4.699850277975202, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 884/1100, Loss: 4.697544876486063, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 885/1100, Loss: 4.699254034087062, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 886/1100, Loss: 4.696249656379223, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 887/1100, Loss: 4.695651736110449, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 888/1100, Loss: 4.697389021515846, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 889/1100, Loss: 4.697203474119306, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 890/1100, Loss: 4.69503241404891, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 891/1100, Loss: 4.703283913433552, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 892/1100, Loss: 4.693071193993092, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 893/1100, Loss: 4.698865382000804, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 894/1100, Loss: 4.695775251835585, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 895/1100, Loss: 4.690008509904146, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 896/1100, Loss: 4.691715260967612, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 897/1100, Loss: 4.692597137764096, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 898/1100, Loss: 4.69124504737556, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 899/1100, Loss: 4.690241249278188, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 900/1100, Loss: 4.690276380628347, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 901/1100, Loss: 4.689777351915836, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 902/1100, Loss: 4.697535369545221, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 903/1100, Loss: 4.687034608796239, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 904/1100, Loss: 4.69285574182868, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 905/1100, Loss: 4.691640138626099, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 906/1100, Loss: 4.686724586412311, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 907/1100, Loss: 4.689377557486296, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 908/1100, Loss: 4.686574064195156, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 909/1100, Loss: 4.685752527788281, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 910/1100, Loss: 4.686587538570166, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 911/1100, Loss: 4.685467356815934, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 912/1100, Loss: 4.684126518666744, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 913/1100, Loss: 4.684098547324538, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 914/1100, Loss: 4.683689683675766, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 915/1100, Loss: 4.686327800154686, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 916/1100, Loss: 4.684285193681717, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 917/1100, Loss: 4.684395821765065, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 918/1100, Loss: 4.68333126604557, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 919/1100, Loss: 4.68258561193943, Learning rate: 1.6384000000000008e-05\n",
      "Epoch 920/1100, Loss: 4.681949615478516, Learning rate: 6.553600000000004e-06\n",
      "Epoch 921/1100, Loss: 4.666882591322064, Learning rate: 6.553600000000004e-06\n",
      "Epoch 922/1100, Loss: 4.665794285014272, Learning rate: 6.553600000000004e-06\n",
      "Epoch 923/1100, Loss: 4.663835858926177, Learning rate: 6.553600000000004e-06\n",
      "Epoch 924/1100, Loss: 4.662620652467012, Learning rate: 6.553600000000004e-06\n",
      "Epoch 925/1100, Loss: 4.662695767357945, Learning rate: 6.553600000000004e-06\n",
      "Epoch 926/1100, Loss: 4.662725042551756, Learning rate: 6.553600000000004e-06\n",
      "Epoch 927/1100, Loss: 4.662233039736748, Learning rate: 6.553600000000004e-06\n",
      "Epoch 928/1100, Loss: 4.66184170730412, Learning rate: 6.553600000000004e-06\n",
      "Epoch 929/1100, Loss: 4.662258943542838, Learning rate: 6.553600000000004e-06\n",
      "Epoch 930/1100, Loss: 4.661779338493943, Learning rate: 6.553600000000004e-06\n",
      "Epoch 931/1100, Loss: 4.66110322624445, Learning rate: 6.553600000000004e-06\n",
      "Epoch 932/1100, Loss: 4.661723053082824, Learning rate: 6.553600000000004e-06\n",
      "Epoch 933/1100, Loss: 4.661536270752549, Learning rate: 6.553600000000004e-06\n",
      "Epoch 934/1100, Loss: 4.660524358972907, Learning rate: 6.553600000000004e-06\n",
      "Epoch 935/1100, Loss: 4.66023675724864, Learning rate: 6.553600000000004e-06\n",
      "Epoch 936/1100, Loss: 4.660904312506318, Learning rate: 6.553600000000004e-06\n",
      "Epoch 937/1100, Loss: 4.660680864006281, Learning rate: 6.553600000000004e-06\n",
      "Epoch 938/1100, Loss: 4.660260738804936, Learning rate: 6.553600000000004e-06\n",
      "Epoch 939/1100, Loss: 4.660546373575926, Learning rate: 6.553600000000004e-06\n",
      "Epoch 940/1100, Loss: 4.659424839541316, Learning rate: 6.553600000000004e-06\n",
      "Epoch 941/1100, Loss: 4.660173030570149, Learning rate: 6.553600000000004e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 942/1100, Loss: 4.660713808611035, Learning rate: 6.553600000000004e-06\n",
      "Epoch 943/1100, Loss: 4.658739050850272, Learning rate: 6.553600000000004e-06\n",
      "Epoch 944/1100, Loss: 4.662000665441155, Learning rate: 6.553600000000004e-06\n",
      "Epoch 945/1100, Loss: 4.658271254971623, Learning rate: 6.553600000000004e-06\n",
      "Epoch 946/1100, Loss: 4.65961697883904, Learning rate: 6.553600000000004e-06\n",
      "Epoch 947/1100, Loss: 4.65872055478394, Learning rate: 6.553600000000004e-06\n",
      "Epoch 948/1100, Loss: 4.658073319122195, Learning rate: 6.553600000000004e-06\n",
      "Epoch 949/1100, Loss: 4.658656815066934, Learning rate: 6.553600000000004e-06\n",
      "Epoch 950/1100, Loss: 4.658358819782734, Learning rate: 6.553600000000004e-06\n",
      "Epoch 951/1100, Loss: 4.657384280115366, Learning rate: 6.553600000000004e-06\n",
      "Epoch 952/1100, Loss: 4.657511321827769, Learning rate: 6.553600000000004e-06\n",
      "Epoch 953/1100, Loss: 4.657438246533275, Learning rate: 6.553600000000004e-06\n",
      "Epoch 954/1100, Loss: 4.656870720908046, Learning rate: 6.553600000000004e-06\n",
      "Epoch 955/1100, Loss: 4.656776076182723, Learning rate: 6.553600000000004e-06\n",
      "Epoch 956/1100, Loss: 4.6568688582628965, Learning rate: 6.553600000000004e-06\n",
      "Epoch 957/1100, Loss: 4.656568616628647, Learning rate: 6.553600000000004e-06\n",
      "Epoch 958/1100, Loss: 4.656879784539342, Learning rate: 6.553600000000004e-06\n",
      "Epoch 959/1100, Loss: 4.660257579758763, Learning rate: 6.553600000000004e-06\n",
      "Epoch 960/1100, Loss: 4.65507273375988, Learning rate: 6.553600000000004e-06\n",
      "Epoch 961/1100, Loss: 4.657618433237076, Learning rate: 6.553600000000004e-06\n",
      "Epoch 962/1100, Loss: 4.656651331111789, Learning rate: 6.553600000000004e-06\n",
      "Epoch 963/1100, Loss: 4.65529166162014, Learning rate: 6.553600000000004e-06\n",
      "Epoch 964/1100, Loss: 4.655747914686799, Learning rate: 6.553600000000004e-06\n",
      "Epoch 965/1100, Loss: 4.654926845803857, Learning rate: 6.553600000000004e-06\n",
      "Epoch 966/1100, Loss: 4.654773326590657, Learning rate: 6.553600000000004e-06\n",
      "Epoch 967/1100, Loss: 4.654662417247891, Learning rate: 6.553600000000004e-06\n",
      "Epoch 968/1100, Loss: 4.654715659096837, Learning rate: 6.553600000000004e-06\n",
      "Epoch 969/1100, Loss: 4.654402416199446, Learning rate: 6.553600000000004e-06\n",
      "Epoch 970/1100, Loss: 4.654669238254428, Learning rate: 6.553600000000004e-06\n",
      "Epoch 971/1100, Loss: 4.658004624769092, Learning rate: 6.553600000000004e-06\n",
      "Epoch 972/1100, Loss: 4.6529589630663395, Learning rate: 6.553600000000004e-06\n",
      "Epoch 973/1100, Loss: 4.65539319626987, Learning rate: 6.553600000000004e-06\n",
      "Epoch 974/1100, Loss: 4.654442589730024, Learning rate: 6.553600000000004e-06\n",
      "Epoch 975/1100, Loss: 4.653060652315617, Learning rate: 6.553600000000004e-06\n",
      "Epoch 976/1100, Loss: 4.653587218374014, Learning rate: 6.553600000000004e-06\n",
      "Epoch 977/1100, Loss: 4.652774505317211, Learning rate: 6.553600000000004e-06\n",
      "Epoch 978/1100, Loss: 4.652611959725618, Learning rate: 6.553600000000004e-06\n",
      "Epoch 979/1100, Loss: 4.652483865618706, Learning rate: 6.553600000000004e-06\n",
      "Epoch 980/1100, Loss: 4.652265237644315, Learning rate: 6.553600000000004e-06\n",
      "Epoch 981/1100, Loss: 4.652294838801026, Learning rate: 6.553600000000004e-06\n",
      "Epoch 982/1100, Loss: 4.652135195210576, Learning rate: 6.553600000000004e-06\n",
      "Epoch 983/1100, Loss: 4.652275955304503, Learning rate: 6.553600000000004e-06\n",
      "Epoch 984/1100, Loss: 4.655793784186244, Learning rate: 6.553600000000004e-06\n",
      "Epoch 985/1100, Loss: 4.650840552523732, Learning rate: 6.553600000000004e-06\n",
      "Epoch 986/1100, Loss: 4.653002720326185, Learning rate: 6.553600000000004e-06\n",
      "Epoch 987/1100, Loss: 4.652193810790777, Learning rate: 6.553600000000004e-06\n",
      "Epoch 988/1100, Loss: 4.65049490891397, Learning rate: 6.553600000000004e-06\n",
      "Epoch 989/1100, Loss: 4.651480900123715, Learning rate: 6.553600000000004e-06\n",
      "Epoch 990/1100, Loss: 4.650763418525457, Learning rate: 6.553600000000004e-06\n",
      "Epoch 991/1100, Loss: 4.650510128587484, Learning rate: 6.553600000000004e-06\n",
      "Epoch 992/1100, Loss: 4.650144338607788, Learning rate: 6.553600000000004e-06\n",
      "Epoch 993/1100, Loss: 4.650959983468056, Learning rate: 6.553600000000004e-06\n",
      "Epoch 994/1100, Loss: 4.650609502568841, Learning rate: 6.553600000000004e-06\n",
      "Epoch 995/1100, Loss: 4.649423558264971, Learning rate: 6.553600000000004e-06\n",
      "Epoch 996/1100, Loss: 4.6528264079242945, Learning rate: 6.553600000000004e-06\n",
      "Epoch 997/1100, Loss: 4.64925142377615, Learning rate: 6.553600000000004e-06\n",
      "Epoch 998/1100, Loss: 4.649302991107106, Learning rate: 6.553600000000004e-06\n",
      "Epoch 999/1100, Loss: 4.649782827124, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1000/1100, Loss: 4.64844954572618, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1001/1100, Loss: 4.648249952122569, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1002/1100, Loss: 4.648320123553276, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1003/1100, Loss: 4.6481495462358, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1004/1100, Loss: 4.647916626185179, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1005/1100, Loss: 4.6479192692786455, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1006/1100, Loss: 4.649447867646813, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1007/1100, Loss: 4.647649481892586, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1008/1100, Loss: 4.649180170148611, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1009/1100, Loss: 4.647289991378784, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1010/1100, Loss: 4.646547798067331, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1011/1100, Loss: 4.647593911737204, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1012/1100, Loss: 4.648045612499118, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1013/1100, Loss: 4.6460874024778605, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1014/1100, Loss: 4.649452347308397, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1015/1100, Loss: 4.645444296300411, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1016/1100, Loss: 4.6462830156087875, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1017/1100, Loss: 4.645603984594345, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1018/1100, Loss: 4.645484356209636, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1019/1100, Loss: 4.645664418116212, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1020/1100, Loss: 4.645265778526664, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1021/1100, Loss: 4.644705485552549, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1022/1100, Loss: 4.64471435919404, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1023/1100, Loss: 4.645211627706885, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1024/1100, Loss: 4.645265994593501, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1025/1100, Loss: 4.64395048096776, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1026/1100, Loss: 4.644655030220747, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1027/1100, Loss: 4.647844897583127, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1028/1100, Loss: 4.642977913841605, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1029/1100, Loss: 4.645158564671874, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1030/1100, Loss: 4.644558610394597, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1031/1100, Loss: 4.642670521512628, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1032/1100, Loss: 4.643733877688646, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1033/1100, Loss: 4.643036745488644, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1034/1100, Loss: 4.642772696912289, Learning rate: 6.553600000000004e-06\n",
      "Epoch 1035/1100, Loss: 4.642426488921046, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1036/1100, Loss: 4.635498680174351, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1037/1100, Loss: 4.635370025411248, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1038/1100, Loss: 4.63576565682888, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1039/1100, Loss: 4.635635951533914, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1040/1100, Loss: 4.635144162923098, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1041/1100, Loss: 4.635017368942499, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1042/1100, Loss: 4.6349929347634315, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1043/1100, Loss: 4.6348435040563345, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1044/1100, Loss: 4.635176273062825, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1045/1100, Loss: 4.635123938322067, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1046/1100, Loss: 4.634628016501665, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1047/1100, Loss: 4.634538136422634, Learning rate: 2.6214400000000015e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1048/1100, Loss: 4.634504439309239, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1049/1100, Loss: 4.634416464716196, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1050/1100, Loss: 4.634414141997695, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1051/1100, Loss: 4.634315814822912, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1052/1100, Loss: 4.634254455566406, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1053/1100, Loss: 4.634232206270099, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1054/1100, Loss: 4.633985817432404, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1055/1100, Loss: 4.634037617594004, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1056/1100, Loss: 4.633866725489497, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1057/1100, Loss: 4.634147219359875, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1058/1100, Loss: 4.634324133396149, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1059/1100, Loss: 4.6335359942168, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1060/1100, Loss: 4.633977131918073, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1061/1100, Loss: 4.633627871051431, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1062/1100, Loss: 4.6333385445177555, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1063/1100, Loss: 4.633316688239574, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1064/1100, Loss: 4.633296435698867, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1065/1100, Loss: 4.633310709148645, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1066/1100, Loss: 4.633327651768923, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1067/1100, Loss: 4.63461404107511, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1068/1100, Loss: 4.6326912101358175, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1069/1100, Loss: 4.633171806111932, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1070/1100, Loss: 4.632836749777198, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1071/1100, Loss: 4.633045516908169, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1072/1100, Loss: 4.632946314290166, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1073/1100, Loss: 4.632539831101894, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1074/1100, Loss: 4.632460746914148, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1075/1100, Loss: 4.63253366574645, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1076/1100, Loss: 4.632478281855583, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1077/1100, Loss: 4.632480431348085, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1078/1100, Loss: 4.632542284205556, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1079/1100, Loss: 4.63214809820056, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1080/1100, Loss: 4.632130915299058, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1081/1100, Loss: 4.632157752290368, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1082/1100, Loss: 4.632125662639737, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1083/1100, Loss: 4.632176358252764, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1084/1100, Loss: 4.631696240976453, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1085/1100, Loss: 4.6319151930511, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1086/1100, Loss: 4.63335070386529, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1087/1100, Loss: 4.631297765299678, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1088/1100, Loss: 4.632034553214908, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1089/1100, Loss: 4.631700284779072, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1090/1100, Loss: 4.631252100691199, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1091/1100, Loss: 4.631242023780942, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1092/1100, Loss: 4.63119019754231, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1093/1100, Loss: 4.6310899425297976, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1094/1100, Loss: 4.631429690867662, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1095/1100, Loss: 4.631003063172102, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1096/1100, Loss: 4.631035165861249, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1097/1100, Loss: 4.6310688983649015, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1098/1100, Loss: 4.6313243098556995, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1099/1100, Loss: 4.630784230306745, Learning rate: 2.6214400000000015e-06\n",
      "Epoch 1100/1100, Loss: 4.630624413490295, Learning rate: 2.6214400000000015e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from ase import Atoms\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to build the graph\n",
    "def build_graph(atoms, cutoff=5.0):\n",
    "    # Compute distances between atoms\n",
    "    distances = atoms.get_all_distances()\n",
    "    \n",
    "    # Get atomic numbers and reshape for concatenation\n",
    "    atomic_numbers = atoms.get_atomic_numbers().reshape(-1, 1).astype(float)\n",
    "    \n",
    "    # Concatenate atomic numbers and distances as features\n",
    "    features = np.hstack([atomic_numbers, distances])\n",
    "    \n",
    "    # Create adjacency matrix: 1 if distance is below cutoff, 0 otherwise (and set diagonal to 0)\n",
    "    adjacency_matrix = (distances < cutoff).astype(float)\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(adjacency_matrix, dtype=torch.float32)\n",
    "\n",
    "# Define the flexible Graph Neural Network (GNN) class\n",
    "class FlexibleGNN(nn.Module):\n",
    "    def __init__(self, input_dim, layers):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add linear layers with ReLU activation functions\n",
    "        last_dim = input_dim\n",
    "        for layer_dim in layers:\n",
    "            self.layers.append(nn.Linear(last_dim, layer_dim))\n",
    "            last_dim = layer_dim\n",
    "        \n",
    "        # Output layer: predicting energy (single scalar)\n",
    "        self.out_layer = nn.Linear(last_dim, 1)\n",
    "\n",
    "    # Forward pass through the network\n",
    "    def forward(self, x, adj_matrix):\n",
    "        for layer in self.layers:\n",
    "            x = torch.matmul(adj_matrix, x)\n",
    "            x = nn.ReLU()(layer(x))  # Apply ReLU activation function\n",
    "        # Output layer\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "# Function to compute energies\n",
    "def compute_energy(atoms, cutoff=5.0):\n",
    "    # Compute distances between atoms\n",
    "    distances = atoms.get_all_distances()\n",
    "    \n",
    "    # Get atomic numbers\n",
    "    atomic_numbers = atoms.get_atomic_numbers()\n",
    "    Z1 = atomic_numbers.reshape(-1, 1)\n",
    "    Z2 = atomic_numbers.reshape(1, -1)\n",
    "    \n",
    "    # Compute interaction values\n",
    "    interaction_values = Z1 * Z2 * np.sin(distances) / distances\n",
    "    np.fill_diagonal(interaction_values, 0)\n",
    "    \n",
    "    # Compute energies\n",
    "    energies = np.sum(np.where(distances < cutoff, interaction_values, 0), axis=1)\n",
    "    \n",
    "    return energies\n",
    "\n",
    "# Test the function\n",
    "positions = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "atoms = Atoms('H2O2', positions=positions)\n",
    "energies = compute_energy(atoms, cutoff=5.0)\n",
    "print(energies)\n",
    "\n",
    "# Construct dataset\n",
    "from ase import Atoms\n",
    "import numpy as np\n",
    "\n",
    "# Function to shake atom positions\n",
    "def shake_positions(positions, scale=0.1):\n",
    "    return positions + np.random.normal(scale=scale, size=positions.shape)\n",
    "\n",
    "# Initial positions for H2O\n",
    "initial_positions = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "\n",
    "# Number of atoms to add in each step\n",
    "num_atoms_per_step = 2\n",
    "\n",
    "# Number of steps to generate 30 atoms\n",
    "num_steps = 15\n",
    "\n",
    "# Create an empty list to store the trajectory\n",
    "trajectory = []\n",
    "\n",
    "# Add the initial configuration\n",
    "atoms = Atoms('H2O2', positions=initial_positions)\n",
    "trajectory.append(atoms)\n",
    "\n",
    "# Generate additional configurations\n",
    "for _ in range(num_steps):\n",
    "    # Copy the previous configuration and shake the positions of the H2O atoms\n",
    "    new_positions = shake_positions(trajectory[-1].positions[0:4], scale=0.1)\n",
    "    \n",
    "    # Create new atoms object with the shaken positions\n",
    "    new_atoms = Atoms('H2O2', positions=np.vstack([trajectory[-1].positions[4:], new_positions]))\n",
    "    \n",
    "    # Append to the trajectory\n",
    "    trajectory.append(new_atoms)\n",
    "\n",
    "# Check the total number of atoms in the trajectory\n",
    "total_atoms = sum(len(atoms) for atoms in trajectory)\n",
    "print(\"Total number of atoms in the trajectory:\", total_atoms)\n",
    "\n",
    "#\n",
    "\n",
    "      \n",
    "data_list = []\n",
    "for atoms in trajectory:\n",
    "    node_features, adj_matrix = build_graph(atoms, cutoff=5.0)\n",
    "    energies = compute_energy(atoms, cutoff=5.0)\n",
    "    data_list.append((node_features, adj_matrix, torch.tensor(energies, dtype=torch.float32)))\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Define and instantiate the FlexibleGNN model\n",
    "num_atoms = len(atoms)\n",
    "input_dim = num_atoms + 1  # +1 for the atomic number\n",
    "layers = [32,64,32]\n",
    "gnn = FlexibleGNN(input_dim=input_dim, layers=layers)\n",
    "print(\"this is the gnn\",gnn)\n",
    "# Set the learning rate and define the optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(gnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Define the scheduler\n",
    "# This will multiply the learning rate by 0.1 every 100 epochs\n",
    "scheduler = StepLR(optimizer, step_size=115, gamma=0.4)\n",
    "# Training loop\n",
    "num_epochs = 1100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for node_features, adj_matrix, target_energies in data_list:\n",
    "        # Forward pass\n",
    "        outputs = gnn(node_features, adj_matrix)\n",
    "        loss = criterion(outputs, target_energies.unsqueeze(-1))  # make target_energies two-dimensional\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print the averaged loss for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_list)}, Learning rate: {scheduler.get_last_lr()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35c3cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 51.99882510304451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuau_\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained GNN model named 'gnn', and test trajectory files named 'test_trajectory_files'\n",
    "# Create an empty list to store the trajectory\n",
    "test_trajectory_files = []\n",
    "\n",
    "# Add the initial configuration\n",
    "atoms = Atoms('H2O2', positions=initial_positions)\n",
    "test_trajectory_files.append(atoms)\n",
    "\n",
    "# Generate additional configurations\n",
    "for _ in range(num_steps):\n",
    "    # Copy the previous configuration and shake the positions of the H2O atoms\n",
    "    new_positions = shake_positions(trajectory[-1].positions[0:4], scale=0.1)\n",
    "    \n",
    "    # Create new atoms object with the shaken positions\n",
    "    new_atoms = Atoms('H2O2', positions=np.vstack([trajectory[-1].positions[4:], new_positions]))\n",
    "    \n",
    "    # Append to the trajectory\n",
    "    test_trajectory_files.append(new_atoms)\n",
    "\n",
    "# 1. Load your test trajectory files and generate test data\n",
    "test_data_list = []\n",
    "for test_traj_file in test_trajectory_files:\n",
    "    test_atoms = test_traj_file\n",
    "    test_node_features, test_adj_matrix = build_graph(test_atoms, cutoff=5.0)\n",
    "    test_energies = compute_energy(test_atoms, cutoff=5.0)\n",
    "    test_data_list.append((test_node_features, test_adj_matrix, torch.tensor(test_energies, dtype=torch.float32)))\n",
    "\n",
    "# 2. Evaluate the model on the test data\n",
    "test_losses = []\n",
    "for node_features, adj_matrix, target_energies in test_data_list:\n",
    "    test_outputs = gnn(node_features, adj_matrix)\n",
    "    test_loss = criterion(test_outputs, target_energies.unsqueeze(-1))\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "# 3. Print the average test loss\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(\"Average test loss:\", avg_test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e69f91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare lists to store predicted and real energy values\n",
    "predicted_energies = []\n",
    "real_energies = []\n",
    "\n",
    "# Pass the test data through the trained GNN model to obtain predictions\n",
    "for node_features, adj_matrix, target_energies in test_data_list:\n",
    "    test_outputs = gnn(node_features, adj_matrix)\n",
    "    predicted_energies.extend(test_outputs.detach().numpy().flatten())  # Convert torch tensor to numpy array\n",
    "    real_energies.extend(target_energies.numpy())\n",
    "\n",
    "# Plot predicted energy vs real energy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(real_energies, predicted_energies, color='blue', alpha=0.5)\n",
    "plt.plot(real_energies, real_energies, color='red', linestyle='--')  # Plot y = x line for reference\n",
    "plt.xlabel('Real Energy')\n",
    "plt.ylabel('Predicted Energy')\n",
    "plt.title('Predicted Energy vs Real Energy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba40151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.collections import g2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996e01bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PH3', 'P2', 'CH3CHO', 'H2COH', 'CS', 'OCHCHO', 'C3H9C', 'CH3COF', 'CH3CH2OCH3', 'HCOOH', 'HCCl3', 'HOCl', 'H2', 'SH2', 'C2H2', 'C4H4NH', 'CH3SCH3', 'SiH2_s3B1d', 'CH3SH', 'CH3CO', 'CO', 'ClF3', 'SiH4', 'C2H6CHOH', 'CH2NHCH2', 'isobutene', 'HCO', 'bicyclobutane', 'LiF', 'Si', 'C2H6', 'CN', 'ClNO', 'S', 'SiF4', 'H3CNH2', 'methylenecyclopropane', 'CH3CH2OH', 'F', 'NaCl', 'CH3Cl', 'CH3SiH3', 'AlF3', 'C2H3', 'ClF', 'PF3', 'PH2', 'CH3CN', 'cyclobutene', 'CH3ONO', 'SiH3', 'C3H6_D3h', 'CO2', 'NO', 'trans-butane', 'H2CCHCl', 'LiH', 'NH2', 'CH', 'CH2OCH2', 'C6H6', 'CH3CONH2', 'cyclobutane', 'H2CCHCN', 'butadiene', 'C', 'H2CO', 'CH3COOH', 'HCF3', 'CH3S', 'CS2', 'SiH2_s1A1d', 'C4H4S', 'N2H4', 'OH', 'CH3OCH3', 'C5H5N', 'H2O', 'HCl', 'CH2_s1A1d', 'CH3CH2SH', 'CH3NO2', 'Cl', 'Be', 'BCl3', 'C4H4O', 'Al', 'CH3O', 'CH3OH', 'C3H7Cl', 'isobutane', 'Na', 'CCl4', 'CH3CH2O', 'H2CCHF', 'C3H7', 'CH3', 'O3', 'P', 'C2H4', 'NCCN', 'S2', 'AlCl3', 'SiCl4', 'SiO', 'C3H4_D2d', 'H', 'COF2', '2-butyne', 'C2H5', 'BF3', 'N2O', 'F2O', 'SO2', 'H2CCl2', 'CF3CN', 'HCN', 'C2H6NH', 'OCS', 'B', 'ClO', 'C3H8', 'HF', 'O2', 'SO', 'NH', 'C2F4', 'NF3', 'CH2_s3B1d', 'CH3CH2Cl', 'CH3COCl', 'NH3', 'C3H9N', 'CF4', 'C3H6_Cs', 'Si2H6', 'HCOOCH3', 'O', 'CCH', 'N', 'Si2', 'C2H6SO', 'C5H8', 'H2CF2', 'Li2', 'CH2SCH2', 'C2Cl4', 'C3H4_C3v', 'CH3COCH3', 'F2', 'CH4', 'SH', 'H2CCO', 'CH3CH2NH2', 'Li', 'N2', 'Cl2', 'H2O2', 'Na2', 'BeH', 'C3H4_C2v', 'NO2']\n"
     ]
    }
   ],
   "source": [
    "print(g2.names)  \n",
    "\"C6 H15 N2 O2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "161e19ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x29587da2240>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ase.build import molecule\n",
    "atoms = read('C:/Users/cuau_/Downloads/protein.xyz')\n",
    "view(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17f6c500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Atoms(symbols='O2N2C6H14', pbc=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217b6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae1aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09bb8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total energy: 136.04171752929688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_total_energy(atoms, gnn_model, cutoff=5.0):\n",
    "    # Step 1: Convert Atoms object into node features and adjacency matrix\n",
    "    node_features, adj_matrix = build_graph(atoms, cutoff=cutoff)\n",
    "    \n",
    "    # Step 2: Pass node features and adjacency matrix through the GNN model\n",
    "    predicted_energies = gnn_model(node_features, adj_matrix)\n",
    "    \n",
    "    # Step 3: Sum up the predicted energies to get the total energy\n",
    "    total_energy = torch.sum(predicted_energies).item()\n",
    "    \n",
    "    return total_energy\n",
    "\n",
    "# Example usage:\n",
    "gnn_model = gnn\n",
    "total_energy = calculate_total_energy(atoms, gnn)\n",
    "print(\"Total energy:\", total_energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b8b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c8ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
